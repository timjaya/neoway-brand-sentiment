{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Benepar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "[nltk_data] Downloading package benepar_en2 to\n",
      "[nltk_data]     C:\\Users\\jyzho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package benepar_en2 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import benepar\n",
    "benepar.download(\"benepar_en2\")\n",
    "parser = benepar.Parser(\"benepar_en2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(reviews, brandlist, sample_size=2000, validation_size=0.1, \n",
    "               test_size=0.25, verbose=0, **kwargs):\n",
    "    \"\"\"Function that generates the dataset for Spacy training. \n",
    "    \n",
    "    Input: Yelp dataset\n",
    "    Output: train/test CSV for ER model training\n",
    "    Parameters:\n",
    "    - reviews: pandas dataframe of reviews\n",
    "    - brandlist: pandas dataframe containing list of products/brands\n",
    "    - sample_size: total number of reviews to subset\n",
    "    - validation_size: proportion of total sample_size to validate on\n",
    "    - test_size: proportion of total sample_size that will serve as the test set\n",
    "    \n",
    "    NOTE \n",
    "    ----\n",
    "    config.data_path: workspace/data\n",
    "    You should use workspace/data to put data to working on.  Let's say\n",
    "    you have workspace/data/iris.csv, which you downloaded from:\n",
    "    https://archive.ics.uci.edu/ml/datasets/iris. You will generate\n",
    "    the following:\n",
    "    + workspace/data/test.csv\n",
    "    + workspace/data/train.csv\n",
    "    + workspace/data/validation.csv\n",
    "    + other files\n",
    "    With these files you can train your model!\n",
    "    \"\"\"\n",
    "    if verbose == 1:\n",
    "      print(\"==> GENERATING DATASETS FOR TRAINING YOUR MODEL\")\n",
    "\n",
    "    # Convert brands in brand list to lowercase\n",
    "    brandlist.word = brandlist.word.str.lower()\n",
    "\n",
    "    # Extract a sample of reviews to generate training/validation/test data from\n",
    "    sample = reviews.sample(n=sample_size)\n",
    "\n",
    "    # Convert reviews to format relevant for spacy training\n",
    "    if verbose == 1:\n",
    "      print(\"   ===> CONVERTING DATA FOR SPACY\")\n",
    "    train_data = []\n",
    "    print(\"LENGTH OF DATASET: \", len(sample))\n",
    "    for index, row in tqdm(sample.iterrows()):\n",
    "        # print(index)\n",
    "        brands_tmp = []\n",
    "        for brand in brandlist.word:\n",
    "            text = row.text.lower()\n",
    "            start_index = 0\n",
    "            while start_index < len(text):\n",
    "                start_index = text.find(brand, start_index)\n",
    "                end_index = start_index + len(brand)\n",
    "                if start_index == -1:\n",
    "                    break\n",
    "                if not text[start_index-1].isalpha() and (end_index == len(text) or not text[end_index].isalpha()):\n",
    "                    if brand not in ['place', 'restaurant', 'cafe', 'establishment', 'diner']:\n",
    "                        brands_tmp.append((start_index, end_index, \"PRODUCT\"))\n",
    "                    else:\n",
    "                        brands_tmp.append((start_index, end_index, \"PRODUCT\"))\n",
    "\n",
    "                start_index += len(brand)\n",
    "        train_data.append((row.review_id, row.text, brands_tmp))\n",
    "\n",
    "    result = pd.DataFrame(train_data, columns=['review_id', 'text', 'entities'])\n",
    "\n",
    "    # Split processed data into train/validation/test sets\n",
    "    if verbose == 1:\n",
    "      print(\"   ===> SPLITTING INTO TRAIN/VALIDATION/TEST SETS\")\n",
    "    train_validation, test = train_test_split(result, test_size=test_size)\n",
    "    train, validation = train_test_split(train_validation, test_size=validation_size / (1-test_size))\n",
    "\n",
    "    # Output to CSV in data folder\n",
    "    train.to_csv('../workspace/data/train.csv')\n",
    "    validation.to_csv('../workspace/data/validation.csv')\n",
    "    test.to_csv('../workspace/data/test.csv')\n",
    "    \n",
    "    if verbose == 1:\n",
    "      print(\"==> DATASETS GENERATED\")\n",
    "    \n",
    "    return train, validation, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training entity recognition model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "import ast \n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def create_train_data(df):\n",
    "    train_data = []\n",
    "    newnlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        doc = newnlp(df['text'].iloc[i])\n",
    "        entity_list = df['entities_clean'].iloc[i]\n",
    "        for ent in doc.ents:\n",
    "            entity_list.append((ent.start_char, ent.end_char, ent.label_))\n",
    "        entity_dict = {\"entities\": entity_list}\n",
    "        train_data.append((df['text'].iloc[i], entity_dict))\n",
    "    return train_data\n",
    "\n",
    "def create_test_data(df):\n",
    "    test_data = []\n",
    "    newnlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        doc = newnlp(df['text'].iloc[i])\n",
    "        entity_list = df['entities_clean'].iloc[i]\n",
    "        for ent in doc.ents:\n",
    "            entity_list.append((ent.start_char, ent.end_char, ent.label_))\n",
    "        entity_dict = {\"entities\": entity_list}\n",
    "        test_data.append((df['text'].iloc[i], entity_dict))\n",
    "    return test_data\n",
    "\n",
    "\n",
    "# new entity label\n",
    "def train(train_data, test_data, LABEL, model='en_core_web_sm', new_model_name=\"product\", output_dir='../ermodel', n_iter=1):\n",
    "    \"\"\"Set up the pipeline and entity recognizer, and train the new entity.\"\"\"\n",
    "    random.seed(0)\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "    # Add entity recognizer to model if it's not in the pipeline\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    ner.add_label(LABEL)  # add new entity label to entity recognizer\n",
    "    if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        optimizer = nlp.resume_training()\n",
    "    move_names = list(ner.move_names)\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        sizes = compounding(1.0, 4.0, 1.001)\n",
    "        # batch the examples using spaCy's minibatch\n",
    "        start = time.time()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(train_data)\n",
    "            batches = minibatch(train_data, size=sizes)\n",
    "            losses = {}\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)            \n",
    "            #print(\"Training Recall:\",nlp.evaluate(random.sample(TRAIN_DATA,200)).ents_r)\n",
    "            #print(\"Test Recall:\",nlp.evaluate(TEST_DATA).ents_p) #COMMENT: isn't this precision?\n",
    "            #COMMENT: so test data here is evaluating test_data which has the format \n",
    "            # of e.g. (\"Uber blew through $1 million a week\", {\"entities\": [(0, 4, \"ORG\")]}) right\n",
    "            #print(\"Training Losses\", losses)\n",
    "        end = time.time()\n",
    "    print(\"Total training time:\",end-start)\n",
    "\n",
    "    # test the trained model (small sample test)\n",
    "    for i in range(10):\n",
    "        test_text = test_data[i][0]\n",
    "        doc = nlp(test_text)\n",
    "        print(\"Entities in '%s'\" % test_text)\n",
    "        for ent in doc.ents:\n",
    "            print(ent.label_, ent.text)\n",
    "\n",
    "    # TODO: Abstract to another function\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.meta[\"name\"] = new_model_name  # rename model\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # COMMENT: Abstract to another function \n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        # Check the classes have loaded back consistently\n",
    "        assert nlp2.get_pipe(\"ner\").move_names == move_names\n",
    "        doc2 = nlp2(test_text)\n",
    "        for ent in doc2.ents:\n",
    "            print(ent.label_, ent.text)\n",
    "    return nlp\n",
    "\n",
    "\n",
    "def run_training(file_name = \"../workspace/data/train.csv\", \n",
    "                 output_dir = '../workspace/models/er_model'):\n",
    "\n",
    "    print(\"   ==> CONFIGURING FORMAT FOR SPACY TRAINING\")\n",
    "\n",
    "    df = pd.read_csv(file_name)\n",
    "    df['entities_clean']=[ast.literal_eval(i) for i in df['entities']]\n",
    "    #train_df, test_df = train_test_split(df, test_size = .2)\n",
    "    all_train, _ = train_test_split(df, train_size=250)\n",
    "    train_df, test_df = train_test_split(all_train, test_size=.2)\n",
    "    \n",
    "    # new entity label\n",
    "    LABEL = \"PRODUCT\"\n",
    "    \n",
    "    TRAIN_DATA = create_train_data(train_df)\n",
    "    TEST_DATA = create_test_data(test_df)\n",
    "\n",
    "    print(\"   ==> TRAINING...\")\n",
    "\n",
    "    model = train(TRAIN_DATA, TEST_DATA, LABEL=LABEL, output_dir=output_dir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_train(**kwargs):\n",
    "    \"\"\"Function that will run your model, be it a NN, Composite indicator\n",
    "    or a Decision tree, you name it.\n",
    "\n",
    "    NOTE\n",
    "    ----\n",
    "    config.models_path: workspace/models\n",
    "    config.data_path: workspace/data\n",
    "\n",
    "    As convention you should use workspace/data to read your dataset,\n",
    "    which was build from generate() step. You should save your model\n",
    "    binary into workspace/models directory.\n",
    "    \"\"\"\n",
    "    print(\"==> TRAINING YOUR SPACY MODEL!\")\n",
    "\n",
    "    # TODO: Load data from workspace/data\n",
    "    # TODO: Save trained model to workspace/models\n",
    "    run_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve Entity Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(nlp_model, text):\n",
    "    \"\"\"\n",
    "    Input nlp_model and text, retrieve a list of unique entities from the text.\n",
    "    \"\"\"\n",
    "    doc = nlp_model(text)\n",
    "    entities = set()\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PRODUCT\":\n",
    "            entities.add(ent.text)\n",
    "    return list(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Helper Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import benepar\n",
    "import re\n",
    "import spacy\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "class Predictor:\n",
    "    def __init__(self,\n",
    "                 sentiment_package = \"vader\",\n",
    "                 parse_package = \"benepar\",\n",
    "                 model_dir = \"../workspace/models/er_model\"):\n",
    "        \n",
    "        self.sentiment_package = sentiment_package\n",
    "        self.nlp = spacy.load(model_dir)\n",
    "        self.num_cores = multiprocessing.cpu_count()\n",
    "        \n",
    "        if parse_package == 'benepar':\n",
    "            try:\n",
    "                self.parser = benepar.Parser(\"benepar_en2\") \n",
    "            except LookupError:\n",
    "                benepar.download('benepar_en2')\n",
    "                self.parser = benepar.Parser(\"benepar_en2\")\n",
    "        elif parse_package == 'stanford':   \n",
    "            pass\n",
    "        else:\n",
    "            raise Exception('incorrect parse package')\n",
    "        \n",
    "    def _remove_nestings(self, lst): \n",
    "        output = []\n",
    "        \n",
    "        def _remove_nestings_recursive(l):\n",
    "            for i in l: \n",
    "                if type(i) == list: \n",
    "                    _remove_nestings_recursive(i) \n",
    "                else: \n",
    "                    output.append(i)\n",
    "        \n",
    "        _remove_nestings_recursive(lst)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def _continue_splitting(self, review, list_of_dividers):    \n",
    "        temp = list_of_dividers.copy()\n",
    "        l = [review]\n",
    "        while len(temp) > 0:\n",
    "            divider = temp.pop(0)\n",
    "            l_new = []\n",
    "            for i in l:\n",
    "                l_new += i.split(divider)\n",
    "            l = l_new\n",
    "        return l\n",
    "    \n",
    "    \n",
    "    def join_clause(self, review, list_of_split_clauses, list_of_dividers):\n",
    "        output = []\n",
    "        loc_of_split_clauses = []\n",
    "        for clause in list_of_split_clauses:\n",
    "            loc_of_split_clauses.append(review.find(clause))\n",
    "        for divider in list_of_dividers:\n",
    "            print(divider)\n",
    "            loc_div = review.find(divider)\n",
    "            print(loc_div)\n",
    "            for i in range(len(loc_of_split_clauses)):\n",
    "                if loc_div > loc_of_split_clauses[i]:\n",
    "                    print(loc_div,loc_of_split_clauses[i])\n",
    "    \n",
    "    \n",
    "    def join_partitions(self, long_review,entity_with_review):\n",
    "        loclist = []\n",
    "        for (_, clause) in entity_with_review:\n",
    "            loclist.append((long_review.find(clause),long_review.find(clause)+len(clause)))\n",
    "        starts = {i for (i,j) in loclist}\n",
    "        ends = {j for (i,j) in loclist}\n",
    "        starts.add(len(long_review))\n",
    "        newends = {}\n",
    "        for i in ends:\n",
    "            newends[i] = min([x for x in starts if x >= i])\n",
    "        for i in newends:\n",
    "            pass\n",
    "        new_entity_with_review = []\n",
    "        for i in range(len(loclist)):\n",
    "            tup = loclist[i]\n",
    "            entity = entity_with_review[i][0]\n",
    "            st = tup[0]\n",
    "            en = newends[tup[1]]\n",
    "            new_entity_with_review.append((entity,long_review[st:en]))\n",
    "        return new_entity_with_review\n",
    "    \n",
    "    \n",
    "    def split_long_string(self, review):\n",
    "        num = len(review)\n",
    "        split_list = []\n",
    "        start = 0\n",
    "        end = 0\n",
    "        while num != end:\n",
    "            #if one step away from end of review\n",
    "            if num - end < 1000:\n",
    "                end = num\n",
    "                split_list.append(review[start:end])\n",
    "            \n",
    "            #otherwise, find the last full stop\n",
    "            else:\n",
    "                end = review[start:(start+1000)].rfind('.') + start\n",
    "                if end == -1:\n",
    "                    end = review[start:(start+1000)].rfind(' ') + start #if no '.', space will do\n",
    "                    if end == -1:\n",
    "                        end = min(start + 1000,num) + start #if there still isn't, then we simply split\n",
    "                split_list.append(review[start:end])\n",
    "                start = end\n",
    "        return(split_list)\n",
    "    \n",
    "    \n",
    "    def split_very_long_string(self, review):\n",
    "        num = len(review)\n",
    "        split_list = []\n",
    "        start = 0\n",
    "        end = 0\n",
    "        while num != end:\n",
    "            #if one step away from end of review\n",
    "            if num - end < 1000:\n",
    "                end = num\n",
    "                split_list.append(review[start:end])\n",
    "            \n",
    "            #otherwise, find the last full stop\n",
    "            else:\n",
    "                end = review[start:(start+400)].rfind('.') + start\n",
    "                if end == -1:\n",
    "                    end = review[start:(start+400)].rfind(' ') + start #if no '.', space will do\n",
    "                    if end == -1:\n",
    "                        end = min(start + 400,num) + start #if there still isn't, then we simply split\n",
    "                split_list.append(review[start:end])\n",
    "                start = end\n",
    "        return(split_list)\n",
    "    \n",
    "    \n",
    "    def split_review_naive(self, review,entities):\n",
    "        clauses = re.split('[.?!]',review)\n",
    "        lenlist = [len(x) for x in clauses]\n",
    "        clauses = [x for _, x in sorted(zip(lenlist,clauses),reverse=False)]\n",
    "        entity_with_clause = []\n",
    "        for entity in entities:\n",
    "            for clause in clauses:\n",
    "                if entity in clause:\n",
    "                    entity_with_clause.append((entity,clause))\n",
    "                    break\n",
    "        return(self.join_partitions(review,entity_with_clause))\n",
    "    \n",
    "    \n",
    "    def min_tree(self, review, entities, output = 'minimum'):\n",
    "        \n",
    "        #review is string, entities is list of strings, parser is parser object\n",
    "        #possible outputs: no_parse, minimum, partition, all\n",
    "        \n",
    "        if output == 'no_parse':\n",
    "            return(self.split_review_naive(review,entities))\n",
    "            \n",
    "        treelist = []\n",
    "        lenlist = []\n",
    "        temp = review.split('\\n')\n",
    "        \n",
    "        if len(review) > 1000:\n",
    "            split_reviews = self.split_long_string(review)\n",
    "        else:\n",
    "            split_reviews = [i for i in temp if len(i) > 1 and len(i) <= 1000 ]\n",
    "        \n",
    "        #if output is partition, we need to keep track of the full review\n",
    "        if output == 'partition':\n",
    "            full_review = ''\n",
    "        \n",
    "        #constituency parsers\n",
    "        \n",
    "        for rev in split_reviews:\n",
    "            if rev and rev.strip():\n",
    "                u = self.parser.parse(rev) # tree \n",
    "    \n",
    "                if type(u) == str:\n",
    "                    u = nltk.Tree.fromstring(u)\n",
    "    \n",
    "                for s in u.subtrees(): # subtrees \n",
    "                    if s.label() == 'S': # if sentence\n",
    "                        treelist += [s]\n",
    "                        lenlist += [len(s.leaves())] # how long clause\n",
    "                            \n",
    "                if output == 'partition':\n",
    "                    full_review += ' '.join(u.leaves()) + ' '\n",
    "    \n",
    "        treelist = [x for _, x in sorted(zip(lenlist,treelist),reverse=False)] # sort by lenlisit\n",
    "        clauses = [' '.join(tree.leaves()) for tree in treelist]\n",
    "        \n",
    "        #If there is no sentences detected, then the full review is the only clause.\n",
    "        if not clauses:\n",
    "            if output == 'partition':\n",
    "                clauses.append(full_review)\n",
    "            else:\n",
    "                clauses.append(review)\n",
    "        entity_with_clause = []\n",
    "        \n",
    "        if output == 'all':\n",
    "            for entity in entities:\n",
    "                clauselist = []\n",
    "                for clause in clauses:\n",
    "                    if entity in clause:\n",
    "                        clauselist.append(clause)\n",
    "                entity_with_clause.append((entity,clauselist))\n",
    "        \n",
    "        #TODO: create rules and test them\n",
    "        elif output == 'minimum':\n",
    "            for entity in entities:\n",
    "                for clause in clauses:\n",
    "                    if entity in clause:\n",
    "                        entity_with_clause.append((entity,clause))\n",
    "                        break\n",
    "                        \n",
    "        elif output == 'partition':\n",
    "            #first find minimal clause\n",
    "            for entity in entities:\n",
    "                for clause in clauses:\n",
    "                    if entity in clause:\n",
    "                        entity_with_clause.append((entity,clause))\n",
    "                        break\n",
    "            #get location of minimal clause in review\n",
    "            \n",
    "            entity_with_clause = self.join_partitions(full_review,entity_with_clause)\n",
    "        \n",
    "        return entity_with_clause\n",
    "    \n",
    "    \n",
    "    def dependency_tree(self, review, entities, output = 'split_min'):\n",
    "        #possible output = split_min, split_all, tree_min, tree_all -> split only uses sentence splitter, while tree takes into account tree structure\n",
    "        doc = self.parser(review)\n",
    "        \n",
    "        if output == 'split_min' or output == 'split_all' or output == 'split_part':\n",
    "            clauses = list(doc.sents)\n",
    "        #length of every clause\n",
    "        \n",
    "        lenlist = [len(str(x)) for x in clauses]\n",
    "            \n",
    "        #sort\n",
    "        clauses = [str(x) for _, x in sorted(zip(lenlist,clauses),reverse=False)]\n",
    "        \n",
    "        \n",
    "        entity_with_clause = []\n",
    "        \n",
    "        if output == 'split_min':\n",
    "            for entity in entities:\n",
    "                for clause in clauses:\n",
    "                    if entity in clause:\n",
    "                        entity_with_clause.append((entity,clause))\n",
    "                        break\n",
    "                        \n",
    "        if output == 'split_all':\n",
    "            for entity in entities:\n",
    "                clauselist = []\n",
    "                for clause in clauses:\n",
    "                    if entity in clause:\n",
    "                        clauselist.append(clause)\n",
    "                entity_with_clause.append((entity,clauselist))\n",
    "        \n",
    "        if output == 'split_part':\n",
    "            for entity in entities:\n",
    "                for clause in clauses:\n",
    "                    if entity in clause:\n",
    "                        entity_with_clause.append((entity,clause))\n",
    "                        break\n",
    "            #get location of minimal clause in review\n",
    "            \n",
    "            entity_with_clause = self.join_partitions(review,entity_with_clause)\n",
    "                \n",
    "        \n",
    "        return(entity_with_clause)\n",
    "\n",
    "\n",
    "    def vader_sentiment(self, entity_with_clause):\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "        entity_with_sentiment = []\n",
    "        for entity, clause in entity_with_clause:\n",
    "            sentiment = analyzer.polarity_scores(clause)['compound']\n",
    "            entity_with_sentiment.append((entity,sentiment))\n",
    "        return(entity_with_sentiment)   \n",
    "\n",
    "\n",
    "    def sentiment_analysis(self, entity_with_review, \n",
    "                           sentiment_package = 'stanford'):\n",
    "        #takes in list of tuples\n",
    "        if sentiment_package == 'stanford':\n",
    "            return stanford_sentiment(entity_with_review)\n",
    "        elif sentiment_package == 'vader':\n",
    "            return self.vader_sentiment(entity_with_review)\n",
    "        else:\n",
    "            raise Exception('incorrect sentiment package')\n",
    "\n",
    "\n",
    "    def sentiment_analysis_indiv(self, clause,sentiment_package = 'stanford'):\n",
    "        #takes in a single review\n",
    "        if sentiment_package == 'stanford':\n",
    "            stanford_sentiment_start()\n",
    "            result = nlp.annotate(clause,\n",
    "                       properties={\n",
    "                           'annotators': 'sentiment',\n",
    "                           'outputFormat': 'json'\n",
    "                       })\n",
    "            return np.dot(result['sentences'][0]['sentimentDistribution'], [-2, -1, 0, 1, 2])\n",
    "        elif sentiment_package == 'vader':\n",
    "            analyzer = SentimentIntensityAnalyzer()\n",
    "            return analyzer.polarity_scores(clause)['compound']\n",
    "        else:\n",
    "            raise Exception('incorrect sentiment package')\n",
    "\n",
    "    \n",
    "    def rule_1(self, review, entities):\n",
    "        entity_with_review = self.min_tree(review, entities, output = 'minimum')\n",
    "        entity_with_sentiment = self.sentiment_analysis(entity_with_review, \n",
    "                                                        self.sentiment_package)\n",
    "        return entity_with_sentiment\n",
    "    \n",
    "    \n",
    "    def rule_2(self, review, entities):\n",
    "        entity_with_review = self.min_tree(review, entities, output = 'all')\n",
    "        entity_with_sentiment = []\n",
    "        sentiment = 0\n",
    "        for ent, revlist in entity_with_review:\n",
    "            for clause in revlist:\n",
    "                sentiment = self.sentiment_analysis_indiv(clause,self.sentiment_package)\n",
    "                if self.sentiment_package == 'vader' and sentiment != 0:\n",
    "                    break\n",
    "                elif self.sentiment_package == 'stanford' and abs(sentiment) > 0.5:\n",
    "                    break\n",
    "                    #if sentiment is not neutral, stop. If sentiment is neutral, keep going up tree.                    \n",
    "            entity_with_sentiment.append((ent,sentiment))\n",
    "        return entity_with_sentiment\n",
    "    \n",
    "    \n",
    "    def rule_3(self, review, entities):\n",
    "        entity_with_review = self.min_tree(review, entities, output = 'all')\n",
    "        \n",
    "        entity_with_sentiment = []\n",
    "        for ent, revlist in entity_with_review:\n",
    "            sentiment_list = []\n",
    "            for clause in revlist:\n",
    "                sentiment = self.sentiment_analysis_indiv(clause,self.sentiment_package)\n",
    "                sentiment_list.append(sentiment)\n",
    "            if not sentiment_list:\n",
    "                sentiment_list.append(0)\n",
    "            entity_with_sentiment.append((ent,np.mean(sentiment_list)))\n",
    "        \n",
    "        return entity_with_sentiment\n",
    "    \n",
    "    \n",
    "    def rule_4(self, review, entities):\n",
    "        entity_with_review = self.min_tree(review, entities, output = 'partition')\n",
    "        entity_with_sentiment = self.sentiment_analysis(entity_with_review, self.sentiment_package)\n",
    "        return entity_with_sentiment\n",
    "    \n",
    "    \n",
    "    def rule_5(self, review, entities):\n",
    "        entity_with_review = self.min_tree(review, entities, output = 'minimum')\n",
    "        entity_with_review_p = self.min_tree(review, entities, output = 'partition')\n",
    "        \n",
    "        entity_with_sentiment = self.sentiment_analysis(entity_with_review, self.sentiment_package)\n",
    "        for i in range(len(entity_with_sentiment)):\n",
    "            sent = entity_with_sentiment[i][1]\n",
    "            if self.sentiment_package == 'vader' and sent != 0:\n",
    "                entity_with_sentiment[i] = (entity_with_sentiment[i][0],\n",
    "                                            self.sentiment_analysis_indiv(entity_with_review_p[i][1],\n",
    "                                                                          self.sentiment_package))\n",
    "            elif self.sentiment_package == 'stanford' and abs(sent) > 0.5:\n",
    "                entity_with_sentiment[i] = (entity_with_sentiment[i][0],\n",
    "                                            self.sentiment_analysis_indiv(entity_with_review_p[i][1],\n",
    "                                                                          self.sentiment_package))\n",
    "    \n",
    "        return entity_with_sentiment\n",
    "    \n",
    "    \n",
    "    def rule_6(self, review, entities):\n",
    "        entity_with_review = self.min_tree(review, entities, output = 'no_parse')\n",
    "        return entity_with_review\n",
    "    \n",
    "    def rule_7(self, review, entities):\n",
    "        self.parser = spacy.load(\"en_core_web_sm\")\n",
    "        entity_with_review = self.dependency_tree(review, entities, output = 'split_min')\n",
    "        entity_with_sentiment = self.sentiment_analysis(entity_with_review, \n",
    "                                                        self.sentiment_package)\n",
    "        return entity_with_sentiment\n",
    "    \n",
    "    def rule_8(self, review, entities):\n",
    "        self.parser = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "        entity_with_review = self.dependency_tree(review, entities, output = 'split_all')\n",
    "        new_entity_with_review = []\n",
    "        entity_with_sentiment = []\n",
    "        sentiment = 0\n",
    "        for ent, revlist in entity_with_review:\n",
    "            for clause in revlist:\n",
    "                sentiment = self.sentiment_analysis_indiv(clause,self.sentiment_package)\n",
    "                if self.sentiment_package == 'vader' and sentiment != 0:\n",
    "                    new_entity_with_review.append((ent,clause))\n",
    "                    break\n",
    "                elif self.sentiment_package == 'stanford' and abs(sentiment) > 0.5:\n",
    "                    new_entity_with_review.append((ent,clause))\n",
    "                    break\n",
    "                    #if sentiment is not neutral, stop. If sentiment is neutral, keep going up tree.                    \n",
    "            entity_with_sentiment.append((ent,sentiment))\n",
    "            \n",
    "        entity_with_review = new_entity_with_review\n",
    "        entity_with_sentiment = self.sentiment_analysis(entity_with_review, \n",
    "                                                        self.sentiment_package)\n",
    "        return entity_with_sentiment \n",
    "        \n",
    "    def rule_9(self, review, entities):\n",
    "        self.parser = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "        entity_with_review = self.dependency_tree(review, entities, output = 'split_min')\n",
    "        entity_with_review_p = self.dependency_tree(review, entities, output = 'split_part')\n",
    "        \n",
    "        entity_with_sentiment = self.sentiment_analysis(entity_with_review, self.sentiment_package)\n",
    "        for i in range(len(entity_with_sentiment)):\n",
    "            sent = entity_with_sentiment[i][1]\n",
    "            if self.sentiment_package == 'vader' and sent != 0:\n",
    "                entity_with_review[i] = entity_with_review_p[i]\n",
    "            elif self.sentiment_package == 'stanford' and abs(sent) > 0.5:\n",
    "                entity_with_review[i] = entity_with_review_p[i]\n",
    "                \n",
    "        return entity_with_sentiment\n",
    "    \n",
    "    \n",
    "    \n",
    "    def kill_host(self):\n",
    "        if self.sentiment_package == \"stanford\":\n",
    "            self.parser.kill_host()\n",
    "        else:\n",
    "            print(\"Stanford server not initialized\")\n",
    "            \n",
    "            \n",
    "    def get_entities(self, text):\n",
    "        \"\"\"\n",
    "        Input nlp_model and text, retrieve a list of unique entities from the text.\n",
    "        \"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        entities = set()\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"PRODUCT\":\n",
    "                entities.add(ent.text)\n",
    "        return list(entities)\n",
    "    \n",
    "    \n",
    "    def _parallelize_default(self, review):\n",
    "        entities = self.get_entities(review)    \n",
    "        result = self.rule_2(review, entities)\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def parallelize_predict(self, input_data):\n",
    "        input_data = self.assert_list_form(input_data)\n",
    "        entities_with_sentiment = Parallel(n_jobs=self.num_cores)(delayed(self._parallelize_default)(i) for i in input_data)\n",
    "        return entities_with_sentiment\n",
    "    \n",
    "    # default using rule 2 for prediction\n",
    "    def defaultPredict(self, input_data):\n",
    "        entities_with_sentiment = []\n",
    "\n",
    "        input_data = self.assert_list_form(input_data)\n",
    "\n",
    "        for review in tqdm(input_data):\n",
    "            entities = self.get_entities(review)    \n",
    "            result = self.rule_2(review, entities)\n",
    "            entities_with_sentiment.append(result)\n",
    "        return entities_with_sentiment\n",
    "        \n",
    "    def assert_list_form(self, input_data):\n",
    "        if not isinstance(input_data, list):\n",
    "            input_data = [input_data]\n",
    "\n",
    "        assert isinstance(input_data, list)\n",
    "        assert isinstance(input_data[0], str) \n",
    "\n",
    "        return input_data\n",
    "    \n",
    "    def customPredict(self, input_data, rule_number=2):\n",
    "        entities_with_sentiment = []\n",
    "\n",
    "        input_data = self.assert_list_form(input_data)\n",
    "\n",
    "        for review in input_data:\n",
    "            entities = self.get_entities(review)\n",
    "            if rule_number == 1:\n",
    "                result = self.rule_1(review, entities)\n",
    "            elif rule_number == 2:\n",
    "                result = self.rule_2(review, entities)\n",
    "            elif rule_number == 3:\n",
    "                result = self.rule_3(review, entities)\n",
    "            elif rule_number == 4:\n",
    "                result = self.rule_4(review, entities)\n",
    "            elif rule_number == 5:\n",
    "                result = self.rule_5(review, entities)\n",
    "            elif rule_number == 6:\n",
    "                result = self.rule_6(review, entities)\n",
    "            elif rule_number == 7:\n",
    "                result = self.rule_7(review, entities)\n",
    "            elif rule_number == 8:\n",
    "                result = self.rule_8(review, entities)\n",
    "            elif rule_number == 9:\n",
    "                result = self.rule_9(review, entities)\n",
    "            else:\n",
    "                raise Exception('Rule number invalid, please choose something between 1 and 9')\n",
    "                \n",
    "            entities_with_sentiment.append(result)\n",
    "            \n",
    "        return entities_with_sentiment\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy validation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"spacy_validate.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1KTr0oUxy27VOldpmjxfs-zf5nGwIwmaf\n",
    "\"\"\"\n",
    "from __future__ import unicode_literals, print_function\n",
    "import ast \n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "import time\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "def create_train_data(df):\n",
    "    train_data = []\n",
    "    newnlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        doc = newnlp(df['text'].iloc[i])\n",
    "        entity_list = df['entities_clean'].iloc[i]\n",
    "        for ent in doc.ents:\n",
    "            entity_list.append((ent.start_char, ent.end_char, ent.label_))\n",
    "            entity_dict = {\"entities\": entity_list}\n",
    "            train_data.append((df['text'].iloc[i], entity_dict))\n",
    "    return train_data\n",
    "\n",
    "def create_test_data(df):\n",
    "    test_data = []\n",
    "    newnlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        doc = newnlp(df['text'].iloc[i])\n",
    "        entity_list = df['entities_clean'].iloc[i]\n",
    "        for ent in doc.ents:\n",
    "            entity_list.append((ent.start_char, ent.end_char, ent.label_))\n",
    "        entity_dict = {\"entities\": entity_list}\n",
    "        test_data.append((df['text'].iloc[i], entity_dict))\n",
    "    return test_data\n",
    "\n",
    "def create_masked_train_data(df, masked_entities):\n",
    "    train_data = []\n",
    "    newnlp = spacy.load(\"en_core_web_sm\")\n",
    "  \n",
    "    for i in range(len(df)):\n",
    "        doc = newnlp(df['text'].iloc[i])\n",
    "        entity_list = df['entities_clean'].iloc[i]\n",
    "        for ent in doc.ents:\n",
    "            if ent.text not in masked_entities:\n",
    "                entity_list.append((ent.start_char, ent.end_char, ent.label_))\n",
    "        entity_dict = {\"entities\": entity_list}\n",
    "        train_data.append((df['text'].iloc[i], entity_dict))\n",
    "    return train_data\n",
    "\n",
    "def masked_train_test(train, test):\n",
    "    brand_list = []\n",
    "    for (index,entity_loc) in enumerate(train['entities_clean']):\n",
    "        text = train['text'].iloc[index]\n",
    "        for pair in entity_loc:\n",
    "            brand_list.append(text[pair[0]:pair[1]])\n",
    "      \n",
    "    import numpy as np\n",
    "    unique_brands = np.unique(brand_list)\n",
    "\n",
    "    newbrand_list = []\n",
    "    for (index, entity_loc) in enumerate(test['entities_clean']):\n",
    "        text = test['text'].iloc[index]\n",
    "        for pair in entity_loc:\n",
    "            newbrand_list.append(text[pair[0]:pair[1]])\n",
    "      \n",
    "    import numpy as np\n",
    "    newunique_brands = np.unique(newbrand_list)\n",
    "\n",
    "    in_common = list(set(unique_brands) & set(newunique_brands))\n",
    "    print(\"Total in common:\",len(in_common))\n",
    "\n",
    "    masked_entities, unmasked_entities = train_test_split(in_common, test_size = .5)\n",
    "    print(\"Total masked:\", len(masked_entities))\n",
    "\n",
    "    # new entity label\n",
    "    TRAIN_DATA = create_masked_train_data(train, masked_entities)\n",
    "    TEST_DATA = create_test_data(test)\n",
    "    return TRAIN_DATA, TEST_DATA, masked_entities, unique_brands, newunique_brands\n",
    "\n",
    "def evaluate_novelty(trained_model, masked_train_data, masked_test_data, masked_entities, unmasked_train_data, unmasked_test_data):\n",
    "    nomask_true = {}\n",
    "    nomask = {}\n",
    "\n",
    "    for review in unmasked_test_data:\n",
    "        test_ents_true = [review[0][start:end] for (start, end, label) in review[1]['entities']]\n",
    "        doc = trained_model(review[0])\n",
    "        test_ents = [ent.text for ent in doc.ents]\n",
    "\n",
    "        for entity in masked_entities:\n",
    "            if entity in test_ents_true: \n",
    "                if (entity in test_ents):\n",
    "                    if entity in nomask.keys():\n",
    "                        nomask[entity] += 1\n",
    "                        nomask_true[entity] +=1\n",
    "                    else: nomask_true[entity] = 0; nomask[entity]=0\n",
    "                elif entity in nomask_true.keys(): nomask_true[entity]+=1\n",
    "                else: nomask_true[entity] = 0\n",
    "\n",
    "    mask_true = {}\n",
    "    mask = {}\n",
    "\n",
    "    for review in masked_test_data:\n",
    "        test_ents_true = [review[0][start:end] for (start, end, label) in review[1]['entities']]\n",
    "        doc = trained_model(review[0])\n",
    "        test_ents = [ent.text for ent in doc.ents]\n",
    "\n",
    "    for entity in masked_entities:\n",
    "        if entity in test_ents_true: \n",
    "            if (entity in test_ents):\n",
    "                if entity in mask.keys():\n",
    "                    mask[entity] += 1\n",
    "                    mask_true[entity] +=1\n",
    "                else: mask_true[entity] = 0; mask[entity]=0\n",
    "            elif entity in mask_true.keys(): mask_true[entity]+=1\n",
    "            else: mask_true[entity] = 0\n",
    "\n",
    "    ratios_without_mask = {}\n",
    "    for key in nomask.keys():\n",
    "        if nomask_true[key] !=0:\n",
    "            ratios_without_mask[key] = nomask[key]/nomask_true[key]\n",
    "    ratios = {}\n",
    "    for key in mask.keys():\n",
    "        if mask_true[key] !=0:\n",
    "            ratios[key] = mask[key]/mask_true[key]\n",
    "\n",
    "    difference = {}\n",
    "    for keys in ratios_without_mask:\n",
    "        difference[keys] =  ratios[keys] - ratios_without_mask[keys]\n",
    "    return difference, ratios, ratios_without_mask\n",
    "\n",
    "def evaluate_spacy(trained_model_dir='../workspace/models/er_model', dataset_path=\"../workspace/data/test.csv\", verbose=True):\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    df['entities_clean']=[ast.literal_eval(i) for i in df['entities']]\n",
    "    train_df, test_df = train_test_split(df, test_size = .2)\n",
    "    trained_model = spacy.load(trained_model_dir)\n",
    "    LABEL = \"PRODUCT\"\n",
    "    masked_TRAIN_DATA, masked_TEST_DATA, masked_entities, unique_brands, newunique_brands = masked_train_test(train_df, test_df)\n",
    "\n",
    "    TRAIN_DATA = create_train_data(train_df)\n",
    "    TEST_DATA = create_test_data(test_df)\n",
    "\n",
    "    difference, ratios, ratios_without_mask = evaluate_novelty(trained_model, masked_TRAIN_DATA,masked_TEST_DATA,masked_entities, TRAIN_DATA,TEST_DATA)\n",
    "    if verbose == True:\n",
    "        print('DIFFERENCES')\n",
    "        print(difference)\n",
    "        print('RATIOS WITH MASK')\n",
    "        print(ratios)\n",
    "        print('RATIOS WITHOUT MASK')\n",
    "        print(ratios_without_mask)\n",
    "    d = {'difference': difference, 'ratios with mask':ratios,'ratios without mask': ratios_without_mask}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running End-to-End Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of businesses in subset:  142\n",
      "Number of businesses with 3.5-4.5 stars:  57\n"
     ]
    }
   ],
   "source": [
    "# GET ALL RESTAURANTS \n",
    "\n",
    "# import data\n",
    "df_raw = pd.read_json(\"../data/restaurant_reviews_1900k.json\", lines=True)\n",
    "\n",
    "# only get restaurants with many reviews\n",
    "many_reviews = df_raw[['business_id','review_id']].groupby(\"business_id\")['review_id'].nunique()\n",
    "many_reviews = many_reviews[many_reviews > 1000].index # more than 100 reviews\n",
    "df = df_raw[df_raw.business_id.isin(set(many_reviews))]\n",
    "print(\"Number of businesses in subset: \", len(df.business_id.unique()))\n",
    "\n",
    "# only grab restaurants with 3-4 stars\n",
    "business_stars = df[['business_id', 'stars']].groupby('business_id').mean()\n",
    "business_ids_similar_stars= business_stars[\n",
    "    (business_stars.stars >= 3.0) \n",
    "    & (business_stars.stars <= 4.0)].index\n",
    "\n",
    "print(\"Number of businesses with 3.5-4.5 stars: \", len(business_ids_similar_stars.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus = df[df.business_id.isin(set(business_ids_similar_stars[:50]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "brandlist = pd.read_csv('../workspace/data/wordnet_food_beverages_list.csv', header=None, names=['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = bus\n",
    "business_ids_similar_stars = bus.business_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> GENERATING DATASETS FOR TRAINING YOUR MODEL\n",
      "   ===> CONVERTING DATA FOR SPACY\n",
      "LENGTH OF DATASET:  1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bbb24a4e8b84213811f41c0d6f317f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ===> SPLITTING INTO TRAIN/VALIDATION/TEST SETS\n",
      "==> DATASETS GENERATED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                  review_id  \\\n",
       " 873  2ZpJNXaqYUGoRkkkoQ49ZQ   \n",
       " 87   r-_e7xOO3Vm8icEP-xtThg   \n",
       " 97   Usv0AqMza-a2r-YX9LJvVg   \n",
       " 819  2s1_hTX2lexfHFlKvXEC-g   \n",
       " 894  0eW0QCLr79rriOUCWTUztg   \n",
       " ..                      ...   \n",
       " 603  fnfShJqwlsej62jYSOPbKA   \n",
       " 168  axy5zMbLpptJKQPKWkOuww   \n",
       " 66   O6yqb3xm1CIRXIw5poOnKw   \n",
       " 477  X_e96eVHnZRz1NWNq7OjwQ   \n",
       " 932  DUQn774cdjhDJRcrpRuLaw   \n",
       " \n",
       "                                                   text  \\\n",
       " 873  I am pretty happy with the service we received...   \n",
       " 87   Beer is served perfectly and the band is amazi...   \n",
       " 97   -Great food\\n-Great staff\\n-The family photo b...   \n",
       " 819  Tasty bagels, sandwiches and desserts.  Kind o...   \n",
       " 894  This is one of my favorite hotel in Las Vegas....   \n",
       " ..                                                 ...   \n",
       " 603  Killer atmosphere, great staff and even better...   \n",
       " 168  The nachos were HUGE they were pretty good. Th...   \n",
       " 66   Loved this buffet!  This is perhaps one of the...   \n",
       " 477  Food was excellent as always. Chicken was seas...   \n",
       " 932  We enjoyed this restaurant a lot. The greeter ...   \n",
       " \n",
       "                                               entities  \n",
       " 873  [(27, 34, PRODUCT), (57, 62, PRODUCT), (95, 10...  \n",
       " 87              [(0, 4, PRODUCT), (179, 184, PRODUCT)]  \n",
       " 97               [(19, 24, PRODUCT), (7, 11, PRODUCT)]  \n",
       " 819  [(483, 488, PRODUCT), (129, 134, PRODUCT), (17...  \n",
       " 894                                                 []  \n",
       " ..                                                 ...  \n",
       " 603  [(25, 30, PRODUCT), (7, 17, PRODUCT), (47, 56,...  \n",
       " 168                                                 []  \n",
       " 66   [(218, 221, PRODUCT), (388, 394, PRODUCT), (21...  \n",
       " 477                                [(30, 37, PRODUCT)]  \n",
       " 932  [(354, 360, PRODUCT), (532, 539, PRODUCT), (60...  \n",
       " \n",
       " [650 rows x 3 columns],                   review_id  \\\n",
       " 882  oIlT1ETMRsT7bZxaooLT8g   \n",
       " 737  eHjUfj6N9J1FuCxVQKL0GA   \n",
       " 539  EpfjwM00K2USWvBiUm3Bvw   \n",
       " 189  TuYee2B_8V6YSYXNyYaQeA   \n",
       " 744  gJs9iFzWz9Svn3mhtLTj6A   \n",
       " ..                      ...   \n",
       " 101  aSjJv9B52Pbl9cdkZ_Nbdg   \n",
       " 841  kUH33QDKoFykLBDDzvi1yg   \n",
       " 803  ccUr7bCTSmFjmDdl1KLQEg   \n",
       " 635  iAwAIJh3ukpYydKFMPbvjQ   \n",
       " 399  p3x-_5ACmVJZ6e49658XwA   \n",
       " \n",
       "                                                   text  \\\n",
       " 882  Yesterday was my first time at this buffet and...   \n",
       " 737  We were going to stay here this last weekend b...   \n",
       " 539  I would give this place 3.5 stars if I could. ...   \n",
       " 189  my husband and I had lunch at the pub and had ...   \n",
       " 744  Oh, look at this, Oh wow look at that, Where d...   \n",
       " ..                                                 ...   \n",
       " 101  I think that for the money, this is my favorit...   \n",
       " 841  Long wait, go early at 10 for lunch. Good choi...   \n",
       " 803  It's at the Rio Hotel. Portions are huge! Did ...   \n",
       " 635  I've only lived in Charlotte for 2 weeks but b...   \n",
       " 399  I had brunch here with my family a few weeks a...   \n",
       " \n",
       "                                               entities  \n",
       " 882             [(36, 42, PRODUCT), (70, 74, PRODUCT)]  \n",
       " 737                              [(438, 444, PRODUCT)]  \n",
       " 539  [(117, 122, PRODUCT), (348, 352, PRODUCT), (51...  \n",
       " 189  [(95, 99, PRODUCT), (121, 128, PRODUCT), (160,...  \n",
       " 744  [(108, 115, PRODUCT), (241, 246, PRODUCT), (11...  \n",
       " ..                                                 ...  \n",
       " 101                              [(361, 368, PRODUCT)]  \n",
       " 841             [(52, 56, PRODUCT), (30, 35, PRODUCT)]  \n",
       " 803  [(315, 320, PRODUCT), (366, 375, PRODUCT), (19...  \n",
       " 635           [(19, 28, PRODUCT), (429, 433, PRODUCT)]  \n",
       " 399  [(165, 172, PRODUCT), (223, 228, PRODUCT), (42...  \n",
       " \n",
       " [100 rows x 3 columns],                   review_id  \\\n",
       " 46   D0kfGR7k3NCcf6hAHzjjyw   \n",
       " 165  fL1PRTGMREVQoZ4PUW7hnA   \n",
       " 573  2CajeNwXQZw8QwCtAIqUvg   \n",
       " 276  3t3qkuOBWtrlfoBdr9gVqw   \n",
       " 402  AQIUxik6crZMnUyxw1BMGg   \n",
       " ..                      ...   \n",
       " 830  93j17up3uklggyMpcmw1og   \n",
       " 424  k1xEMNfTKThiOZDKNXC23w   \n",
       " 605  A_rSXtJERn9mqBuPcWvWjA   \n",
       " 585  LAX6nABQ9VRfJ8X9VcWHXQ   \n",
       " 36   RbapwVYqBFrYsYeC7zdvXw   \n",
       " \n",
       "                                                   text  \\\n",
       " 46   I am late writing this review. One week to be ...   \n",
       " 165  If you want meat, they got it. Skewer after sk...   \n",
       " 573  The food & server was awesome. I had a burger ...   \n",
       " 276  This new establishment is truly a growing one ...   \n",
       " 402  Food was excellent. Wide variety of food and p...   \n",
       " ..                                                 ...   \n",
       " 830  We eat here every time we come to Las Vegas. T...   \n",
       " 424  We ate here on the 23th and our server Jon was...   \n",
       " 605  This was a decent buffet. Went for a weekday b...   \n",
       " 585  We came in with a group of 14 and had a great ...   \n",
       " 36   Just visited the Cravings buffet on a Saturday...   \n",
       " \n",
       "                                               entities  \n",
       " 46   [(723, 728, PRODUCT), (949, 956, PRODUCT), (29...  \n",
       " 165  [(101, 106, PRODUCT), (12, 16, PRODUCT), (78, ...  \n",
       " 573  [(133, 141, PRODUCT), (262, 267, PRODUCT), (4,...  \n",
       " 276  [(827, 834, PRODUCT), (853, 860, PRODUCT), (87...  \n",
       " 402  [(144, 148, PRODUCT), (465, 469, PRODUCT), (71...  \n",
       " ..                                                 ...  \n",
       " 830  [(518, 525, PRODUCT), (560, 567, PRODUCT), (60...  \n",
       " 424  [(106, 110, PRODUCT), (265, 269, PRODUCT), (37...  \n",
       " 605  [(137, 141, PRODUCT), (357, 361, PRODUCT), (24...  \n",
       " 585  [(188, 192, PRODUCT), (254, 258, PRODUCT), (32...  \n",
       " 36   [(67, 72, PRODUCT), (157, 162, PRODUCT), (713,...  \n",
       " \n",
       " [250 rows x 3 columns])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(reviews, brandlist, sample_size=500, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_ids_similar_stars = reviews.business_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> TRAINING YOUR SPACY MODEL!\n",
      "   ==> CONFIGURING FORMAT FOR SPACY TRAINING\n",
      "   ==> TRAINING...\n",
      "Loaded model 'en_core_web_sm'\n",
      "Total training time: 27.65606999397278\n",
      "Entities in 'Probably the best ramen in town. The food is amazing and consistent. If you want solid Japanese Ramen then this is your place. The only downside is the dining area is small so hit it during off hours or be prepared to wait.  Well worth it.'\n",
      "PRODUCT food\n",
      "NORP Japanese\n",
      "PRODUCT place\n",
      "TIME off hours\n",
      "Entities in 'Great food, great price, and amazing service. Pot roast fries are amazing!! I had the belgium waffle this morning and it was the best waffle I've ever had. The burgers are awesome. I literally love everything on the menu.'\n",
      "PRODUCT food\n",
      "PRODUCT price\n",
      "PRODUCT service\n",
      "PRODUCT fries\n",
      "TIME this morning\n",
      "PRODUCT menu\n",
      "Entities in 'Our food was on point. The crab legs were  boiling hot and ready to eat...they already come cracked or sliced in half so no work needed. The prime rib lacked a deep flavored profile. Now for the best part desserts are amazing. There maple bacon toffee pudding is to die for!! Yes maple bacon toffee pudding!! Enjoy'\n",
      "PRODUCT food\n",
      "PRODUCT crab\n",
      "CARDINAL half\n",
      "Entities in 'Vegan station, and lots of vegan options mixed in with other stations. Vegan options are labeled. Beer. Wine. Yay.'\n",
      "GPE Vegan\n",
      "ORG Vegan\n",
      "PERSON Beer\n",
      "PERSON Wine\n",
      "Entities in 'Omg omg this place was awful unbelievable .\n",
      "We sat outside and we were hoping to get nice view to watch the fountain and we got nice table too but as soon as fountain start to begin omg all the adults  and kids are running outside to watch it and u can't even enjoy your food and I was in shock even the parents didn't mention anything to their children or staff should be very concern about this we didn't enjoy food and we couldn't even enjoy the view to many adults  and children were running out side.\n",
      "Food was horrible no taste at all steak and salmon was so so bad and we were so disappointed by this place and I can't even give this place 1star .'\n",
      "PRODUCT place\n",
      "PRODUCT table\n",
      "PRODUCT food\n",
      "PRODUCT staff\n",
      "PRODUCT food\n",
      "PRODUCT side\n",
      "PRODUCT Food\n",
      "PRODUCT taste\n",
      "PRODUCT steak\n",
      "PRODUCT salmon\n",
      "PRODUCT place\n",
      "PRODUCT place\n",
      "CARDINAL 1star\n",
      "Entities in 'Prob my fave place to brunch/lunch on the strip. \n",
      "\n",
      "Between 5 girls and 3 bottles of Cakebread's buttery chard, we shared several items which the waiter recommended:\n",
      "\n",
      "-Beef Carpaccio\n",
      "-Tuna tartare\n",
      "-Lobster bolognese\n",
      "-Rib eye w. a red wine reduction\n",
      "-Fig and prosciutto flatbread\n",
      "we also got a couple items incl this smoked salmon dish (my fave, forgot name) and bread basket with Tapenade (best Tapenade ever).\n",
      "\n",
      "Yumsicles - hope to visit again soon.'\n",
      "PRODUCT place\n",
      "PRODUCT lunch\n",
      "CARDINAL Between 5\n",
      "CARDINAL 3\n",
      "ORG Cakebread\n",
      "PRODUCT bolognese\n",
      "PRODUCT wine\n",
      "PRODUCT flatbread\n",
      "PRODUCT salmon\n",
      "PRODUCT dish\n",
      "PRODUCT bread\n",
      "PERSON Tapenade\n",
      "Entities in 'Good food but a little bit less selection when compared to other high-end buffet in the strip. Love the food quality and make room for desserts. Came in 7pm on Saturday, no line but when we left an hour later, line was long. $44 / adult, $18/ kids. Better price compared to Bellagio or Bacchanal. Overall experience is good, would recommend to others.'\n",
      "PRODUCT food\n",
      "PRODUCT bit\n",
      "PRODUCT buffet\n",
      "PRODUCT food\n",
      "TIME 7pm\n",
      "DATE Saturday\n",
      "TIME an hour later\n",
      "MONEY 44\n",
      "MONEY 18/\n",
      "PRODUCT price\n",
      "PERSON Bellagio\n",
      "Entities in 'I will start with the good stuff and then explain my 3 stars. The food selection was top notch and I can see why they are the no. 1 buffet in Vegas. However it is still over priced. I went to celebrate my birthday with family and for 3 adults at dinner on a weekday, it cost $172. For that, I will go to a high end restaurant. \n",
      "\n",
      "The draw was the raw oysters, king crab legs, large and tasty shrimp. I saw people bypassing the carving station (lamb shank which hubby said was very good, ribs, turkey, beef, sausages) to go straight to the seafood section. \n",
      "\n",
      "The Mexican section had an assortment of juices and horchata (just ok). The Italian was too filling so I skipped it. The cold cut and cheese section was varied and quite good. Then there was a large Asian section, sushi was quite good, Thai was decent, and Chinese, including some dim sum, was good. \n",
      "\n",
      "The dessert bar had fudge and macaroons along with fresh fruit, ice cream and selection of sweets. \n",
      "\n",
      "Now the bad, we experienced very poor service. Any other establishment of equal caliber in Vegas would have immediately offered to comp the meal. However, the manager hemmed and hawed and clearly did not understand customer service. It was only after significant arguing did they agree to credit us. At this point we had lost our appetite.'\n",
      "CARDINAL 3\n",
      "PRODUCT food\n",
      "CARDINAL 1\n",
      "PRODUCT buffet\n",
      "GPE Vegas\n",
      "CARDINAL 3\n",
      "PRODUCT dinner\n",
      "MONEY 172\n",
      "PRODUCT restaurant\n",
      "PRODUCT crab\n",
      "PRODUCT turkey\n",
      "PRODUCT beef\n",
      "PRODUCT seafood\n",
      "NORP Mexican\n",
      "NORP Italian\n",
      "PRODUCT cheese\n",
      "NORP Asian\n",
      "PRODUCT sushi\n",
      "PERSON Thai\n",
      "NORP Chinese\n",
      "PRODUCT dessert\n",
      "PRODUCT cream\n",
      "PRODUCT service\n",
      "GPE Vegas\n",
      "PRODUCT meal\n",
      "PRODUCT service\n",
      "Entities in 'Took the parents here for brunch one Sunday. PRO TIP: make a reservation. Not my first time coming here but it was theirs. Ordered coffee, and a bloody Mary to get started. While the bloody Mary was fantastic it was also fun sized which kind of threw me off. Everyone's coffee was cold which was also kind of a bummer. Other than that the food was killer. Sweet potato pancakes are a must. While the menu may seem pricey, dont worry you get what you pay for and that is quality. Will be back.'\n",
      "DATE Sunday\n",
      "ORDINAL first\n",
      "PRODUCT coffee\n",
      "PERSON Mary\n",
      "PERSON Mary\n",
      "PRODUCT coffee\n",
      "PRODUCT food\n",
      "PERSON Sweet\n",
      "PRODUCT potato\n",
      "PRODUCT menu\n",
      "Entities in 'The nachos are the absolute best. HUGE. As big as they are, the staff does a decent job of making sure that there's enough of the \"good stuff\" so that you aren't left with just a ton of empty sad chips. Make sure to get an extra side of salsa, there's never enough salsa! Everything else that I have ever gotten here is alright, but the nachos are just wonderful. \n",
      "\n",
      "The drinks are strong and delicious. Definitely worth the price. \n",
      "\n",
      "The atmosphere is great. So much fun to be had. Unless you really really don't like Jimmy Buffett music.\n",
      "\n",
      "The wait can be incredibly long for a table though. If you are just looking to grab a drink, they have a counter by the front door.'\n",
      "PRODUCT staff\n",
      "PRODUCT salsa\n",
      "PRODUCT salsa\n",
      "PRODUCT price\n",
      "PRODUCT atmosphere\n",
      "PERSON Jimmy Buffett\n",
      "PRODUCT table\n",
      "Saved model to ..\\workspace\\models\\er_model\n",
      "Loading from ..\\workspace\\models\\er_model\n",
      "PRODUCT staff\n",
      "PRODUCT salsa\n",
      "PRODUCT salsa\n",
      "PRODUCT price\n",
      "PRODUCT atmosphere\n",
      "PERSON Jimmy Buffett\n",
      "PRODUCT table\n"
     ]
    }
   ],
   "source": [
    "# Train spacy model\n",
    "main_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spacy model\n",
    "nlp = spacy.load('..\\workspace\\models\\er_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbfc1b26e8fb4bb99c6e6369073c772b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on restaurant  d_L-rfS1vT3JMzgCUGtiow ...\n",
      "Number of Reviews left after subset length:  679\n",
      "Extracting entities from each review...\n",
      "Filtering entities to have enough reviews...\n",
      "Calculating Yelp Star Rankings... \n",
      "Calculating Prediction Rankings...\n",
      "Performing sentiment analysis for each review... \n",
      "defaultdict(<class 'list'>, {'entity': ['chef', 'planet', 'food', 'service', 'chicken', 'quesadilla', 'bit', 'price', 'dishes', 'salad', 'buffet', 'review', 'Food', 'deal', 'menu', 'fritters', 'skinny', 'sauce', 'meal', 'quesadillas', 'burger', 'bottom', 'nachos', 'guacamole', 'Service', 'Guac', 'Leche', 'restaurant', 'dish', 'taste', 'crab', 'ceviche', 'platter', 'mood', 'coconut', 'plate', 'place', 'downstairs', 'steak', 'bacon', 'soupy', 'Rice', 'slaw', 'sangria', 'salsa', 'fish', 'skirt', 'chips', 'meat', 'shrimp', 'Guacamole', 'spicy', 'breakfast', 'margarita', 'dinner', 'foodie', 'pepper', 'mole', 'brunch', 'sweet', 'chile', 'salsas', 'cream', 'black', 'order', 'suffers', 'staff', 'disgrace', 'empandas', 'kobe', 'chocolate', 'tacos', 'cookies', 'main', 'pleasant', 'prickly', 'splurge', 'joint', 'daily', 'rice', 'cheese', 'Baja', 'carnitas', 'small', 'huitlacoche', 'margaritas', 'broth', 'flavor', 'avocado', 'friend', 'skeezer', 'pork', 'tuna', 'Light', 'octopus', 'seafood', 'freshly', 'manager', 'tortillas', 'sweatshirt', 'rib', 'mahi', 'orange', 'Fish', 'finish', 'banana', 'dessert', 'lunch', 'list', 'entree', 'beef', 'Staff', 'minty', 'sprouts', 'brussel', 'green', 'pozole', 'dished', 'courses', 'bill', 'lamb', 'tortilla', 'Chef', 'lazy', 'watermelon', 'coupon', 'spice', 'regular', 'casino', 'specials', 'risk', 'Sauce', 'corn', 'table', 'corner', 'carne', 'recommendation', 'beer', 'bunch', 'Ceviche', 'system', 'vegan', 'atmosphere', 'cauliflower', 'blanco', 'mixto', 'options', 'ladies', 'tortillasare', 'freshness', 'mojito', 'solid', 'bomb', 'bartender', 'Sustainable', 'side', 'pomegranate', 'special', 'border', 'allergies', 'cervice', 'sandwich', 'prices', 'berry', 'specialty', 'freebie', 'Cheese', 'chop', 'burrito', 'upside', 'Atmosphere', 'selections', 'bottomless', 'mimosas', 'Sangria', 'char', 'buns', 'Highly', 'flour', 'chili', 'upscale', 'soup', 'front', 'potato', 'queso', 'bartenders', 'pescado', 'churro', 'chilaquiles', 'part', 'pinch', 'Cerviche', 'binoculars', 'fondue', 'wine', 'beans', 'quinoa', 'cioppino', 'cotta', 'enchiladas', 'cerviche', 'baja', 'adobo', 'radish', 'free', 'south', 'street', 'plaintain', 'concert', 'beach', 'mandalay', 'patron', 'Mimosas', 'Sweet', 'starter', 'doughy', 'moon', 'bland', 'Restaurant', 'penny', '&', 'Amber', 'flavorless', 'fast', 'apps', 'pricey', 'course', 'cocktail', 'broccolini', 'enchilladas', 'rellenos', 'servers', 'portion', 'yucatan', 'sevechi', 'brisket', 'plantain', 'Green', 'mary', 'portabello', 'sugar', 'salt', 'asada', 'bestie', 'stood', 'decor', 'combo', 'flan', 'juice', 'power', 'buds', 'park', 'Carlie', 'experiance', 'Autumn', 'farm', 'sweets', 'bite', 'scallop', 'conferences', 'rockefeller', 'view', 'marg', 'tecate', 'birthday', 'crossiant', 'flair', 'nice', 'perk', 'blood', 'boyfriend', 'place.may', 'casadia', 'snappy', 'Asada', 'short', 'raspberry', 'Limeade', 'Candice', 'normal', 'Finish', 'tres', 'Tacos', 'taco', 'slice', 'ceviches', 'grapefruit', 'point', 'Solid', 'summer', 'skin', 'flourless', 'truffle', 'outfits', 'sirloin', 'biscuits', 'habanero', 'busboy', 'Fries', 'breast', 'Coworker', 'caeser', 'surprise', 'foods', 'dollar', 'Chicken', 'fork', 'pools', 'pablano', 'relish', 'boar', 'delish', 'resort', 'crunchy', 'conference'], 'predicted_score': [0.3298125, 0.4249666666666667, 0.4587910344827586, 0.5123306878306878, 0.27581199999999995, 0.28226666666666667, 0.18769333333333332, 0.316436, 0.17845, 0.49257499999999993, -0.0008800000000000086, -0.12916666666666668, 0.38544909090909096, 0.6974, 0.2932967213114754, 0.24635, 0.20246666666666666, 0.42145, 0.3520526315789474, 0.24100000000000002, 0.5993999999999999, 0.2382, 0.09154999999999999, 0.5889357142857142, 0.4514863636363637, 0.5719, 0.37895, 0.19260909090909087, 0.46059444444444453, 0.05795, -0.05390000000000002, 0.26965, 0.807, 0.4336333333333333, 0.6249, 0.4120333333333333, 0.43106742424242417, 0.28595, 0.32327307692307694, 0.3825, 0.0, 0.0, 0.36879999999999996, 0.24305714285714286, 0.47724821428571435, 0.20455172413793102, 0.38254, 0.41651199999999994, 0.4429, 0.37895, 0.5450444444444443, 0.5274666666666666, 0.40519, 0.3905, 0.258615, 0.0, 0.5574, 0.5072571428571429, 0.269365, 0.6197238095238096, 0.6249, 0.4102333333333334, 0.3241142857142857, 0.40730000000000005, -0.5267, -0.4767, 0.5466891891891893, -0.5413, -0.5994, 0.9313, 0.42294, -0.0906, 0.05886666666666666, 0.2202, 0.8996500000000001, 0.366, 0.4588, 0.2382, 0.71705, 0.19137272727272728, 0.01543333333333333, 0.6696333333333332, 0.3334625, 0.1468, 0.0, 0.5311619047619047, 0.65795, 0.24635, 0.0, 0.7703249999999999, 0.4404, 0.258672, 0.24635, 0.4404, 0.0, -0.22145, 0.0, 0.18025000000000002, 0.418, 0.5231, 0.33663333333333334, 0.4404, 0.3045857142857143, 0.3807571428571429, 0.69495, 0.0, 0.027250000000000014, 0.19772307692307695, 0.6249, 0.3061, 0.45768000000000003, 0.6585666666666666, 0.0, 0.0, 0.0, 0.43221, 0.0, 0.5574, 0.0, 0.0, -0.03735000000000001, 0.30135, 0.5421999999999999, 0.8003, 0.642875, 0.7177, -0.029349999999999987, 0.3592, 0.0, 0.11315, 0.2263, 0.5859, 0.44499999999999995, 0.09908333333333333, 0.25465, 0.4118666666666667, 0.6791, 0.40080000000000005, 0.1462, 0.4158, 0.0, 0.47446666666666665, 0.5845428571428571, 0.38743333333333335, -0.5106, -0.5106, 0.0, 0.0, 0.0, 0.4939, 0.4095333333333333, 0.3412, 0.6792, 0.27065, 0.0, 0.3247, 0.67115, 0.5997857142857143, 0.30569999999999997, 0.5464666666666667, 0.8126, 0.05437499999999999, 0.39865, -0.1007, -0.6007, 0.6249, 0.3182, 0.0, 0.1631, 0.0, 0.51555, 0.0, 0.35218, 0.464975, 0.7436, 0.1806, 0.0, 0.38677500000000004, 0.5859, 0.0, 0.9209, 0.0, 0.0, 0.8074, 0.181, 0.8122, 0.9547, 0.2787, 0.3348, 0.559075, 0.431, 0.4404, 0.0, 0.7447, 0.2382, -0.1531, 0.0, 0.0, 0.34, 0.47355, 0.186, 0.186, 0.0, 0.2023, 0.5527000000000001, 0.0, 0.0, 0.6114, 0.28396666666666665, 0.0, 0.4215, 0.57165, 0.0, 0.7783, 0.7783, -0.68, -0.4956, 0.4215, 0.0, 0.44005, 0.7003, 0.6269666666666667, 0.0386, 0.5719, 0.0, 0.3837, 0.0, 0.5994, 0.0, 0.8481, 0.8004, 0.4537333333333333, 0.8135, 0.26385, 0.5106, 0.765, 0.5816399999999999, 0.2382, -0.03160000000000002, 0.0, 0.05135, -0.3182, 0.1366, 0.0, -0.296, 0.8658, 0.0, 0.209025, 0.3182, 0.3182, 0.0, 0.0, 0.658, 0.6124, 0.0, 0.8651, 0.4939, 0.6535, 0.4019, 0.8955, 0.0, 0.5232, 0.3182, 0.0, 0.9215, 0.3612, 0.0, 0.6369, 0.7351, 0.31245, 0.886, 0.0, 0.1744, 0.0, 0.0, 0.0, 0.4062, 0.6249, 0.8126, 0.4754, 0.0, 0.0, 0.3612, 0.4019, 0.6369, 0.7184, 0.6369, 0.2263, 0.1531, 0.6356, 0.6369, 0.3182, 0.3182, 0.0, 0.0, 0.0, 0.8176, 0.6705, 0.0, 0.0, 0.0, 0.0, 0.5994, 0.7264, 0.7732, 0.2716, -0.296, 0.4404, 0.0, 0.0, 0.8934, 0.6588, 0.0, 0.3182, 0.0]})\n",
      "Rankings result: \n",
      "        entity  average_stars  predicted_score\n",
      "0         food       3.826979         0.458791\n",
      "1      service       3.904552         0.512331\n",
      "2      chicken       3.864151         0.275812\n",
      "3         menu       4.064815         0.293297\n",
      "4         meal       3.812081         0.352053\n",
      "5   restaurant       3.730858         0.192609\n",
      "6        place       3.782772         0.431067\n",
      "7        salsa       3.706849         0.477248\n",
      "8         fish       3.533333         0.204552\n",
      "9       dinner       3.873874         0.258615\n",
      "10       staff       4.046512         0.546689\n",
      "Spearman Correlation Score:  0.36363636363636365\n",
      "Running on restaurant  N0apJkxIem2E8irTBRKnHw ...\n",
      "Number of Reviews left after subset length:  711\n",
      "Extracting entities from each review...\n",
      "Filtering entities to have enough reviews...\n",
      "Calculating Yelp Star Rankings... \n",
      "Calculating Prediction Rankings...\n",
      "Performing sentiment analysis for each review... \n",
      "defaultdict(<class 'list'>, {'entity': ['concert', 'band', 'food', 'service', 'beer', 'place', 'grill', 'menu', 'crab', 'dinner', 'entree', 'prosciutto', 'pizza', 'calamari', 'price', 'meatball', 'restaurant', 'staff', 'bit', 'pasta', 'sandwich', 'salmon', 'Food', 'burger', 'angus', 'french', 'cheese', 'dish', 'Service', 'chicken', 'meal', 'plate', 'prices', 'bread', 'ricotta', 'Restaurant', 'steakhouse', 'perfect', 'chips', 'casino', 'bartender', 'steak', 'skewers', 'chocolate', 'boar', 'fries', 'salad', 'friend', 'strawberry', 'potato', 'slip', 'STAFF', 'lunch', 'atmosphere', 'ginger', 'meat', 'waitstaff', 'fish', 'breakfast', 'foccacia', 'boyfriend', 'table', 'pain', 'champ', 'berry', 'coast', 'tuna', 'penny', 'manager', 'cocktail', 'dessert', 'special', 'sweet', 'brioche', 'margarita', 'cream', 'bolognese', 'blend', 'vegetarians', 'dishes', 'surrounding', 'nice', 'club', 'fact', 'gnocchi', 'meatloaf', 'shrimp', 'aioli', 'pizzas', 'bistro', 'truffle', 'birthday', 'flat', 'ravioli', 'comparable', 'burgers', 'mushrooms', 'combo', 'cakes', 'linguine', 'pour', 'Caeser', 'clam', 'souffle', 'part', 'marble', 'course', 'godforsaken', 'pork', 'real', 'rib', 'Staff', 'broth', 'portion', 'decor', 'split', 'bite', 'pricing', 'spice', 'cigar', 'side', 'runner', 'problem', 'flower', 'spinach', 'doubt', 'blue', 'runny', 'potatoes', 'comps', 'neighborhoods', 'squash', 'butternut', 'family', 'saltine', 'blonde', 'winner', 'ordinary', 'cucumber', 'meaty', 'patty', 'beyond', 'spaghetti', 'beet', 'hamburger', 'seat', 'servers', 'trip', 'marinara', 'chef', 'starter', 'skewer', 'seasonal', 'puck', 'choice', 'sandwiches', 'milanese', 'Stella', 'blast', 'shiznit', 'space', 'courses', 'Nice', 'waiter', 'mediocre', 'fast', 'refills', 'mayo', 'delish', 'exit', 'colleague', 'upscale', 'pricey', 'beignet', 'sauce', 'temp', 'soup', 'short', 'meatballs', 'lasagna', 'fennel', 'rigatoni', 'fettuccine', 'Light', 'riccotta', 'flirt', 'comedy', '6:00am', 'bartenders', 'juice', 'bass', 'souffle', 'mousse', 'point', 'peppers', 'wine', 'sangria', 'salade', 'mojito', 'buds', 'priscutto', 'raviolis', 'blt', 'bone', 'chop', 'roast', 'brle', 'Pasta', 'Highly', 'busboy', 'platter', 'bachelorette', 'fettuccini', 'seafood', 'bomb', 'scampi', 'main', 'Cut', 'customer', 'pepperoni', 'Cardboard', 'calories', 'sparse', 'Salad', 'appetizer', 'jumbo', 'shrimps', 'Chicken', 'serve', 'Mac', 'noir', 'prime', 'skin', 'ladies', 'cookies', 'macncheese', 'jidori', 'speak', 'diavola', 'polenta', 'Burger', 'corn', 'flavorless', 'pozole', 'spicy', 'Solid', 'Sad', 'pieces', 'buffet', 'mash', 'Price', 'survice', 'puree', 'risotto', 'green', 'caprice', 'brule', 'cauliflower', 'spaghettini', 'brussel', 'mac', 'macNcheese', 'preferences', 'allergies', 'souffl', 'wood', 'stylish', 'busser', 'reuben', 'quesadilla', 'pricy', 'brand', 'shift', 'jack', 'black', 'solid', 'sensitivities', 'costumer', 'romaine', 'Salmon', 'zucchini', 'mood', 'advice', 'bleu', 'airline', 'mentioned', 'simpele', 'crack', 'pastas', 'ceasar', 'Sandwiches', 'coleslaw', 'slot', 'bakes', 'firewood', 'robert', 'properties', 'sprouts', 'bacon', 'formula', 'outcome', 'taste', 'mistake', 'crappy', 'creme', 'pocket', 'donuts', 'sugar', 'focaccia', 'Chef', 'expensive', 'breast', 'casual', 'coarse', 'mussel', 'plane', 'Atmosphere', 'fresno', 'frito', 'pineapple', '200C.', 'sardines', \"d'endroit\", 'russie', 'crust', 'superb', 'prosuitto', 'salami', 'champagne', 'freshness', 'calzone', 'brulle', 'rosemary', 'summer', 'gin', 'favorites', 'barrel', 'clams', 'linguini', 'buck'], 'predicted_score': [0.4158, 0.8316, 0.46520032786885246, 0.5547754464285715, 0.18942142857142857, 0.37741216216216217, 0.6696, 0.3609810810810811, 0.32101071428571426, 0.3577491803278688, 0.3388285714285714, 0.2933333333333334, 0.3247836956521739, 0.43768333333333337, 0.3802884615384616, 0.2601333333333333, 0.3782227272727273, 0.5170558823529412, 0.2435310344827586, 0.34093846153846147, 0.29219999999999996, 0.34517826086956527, 0.3861166666666666, 0.3844392156862744, 0.0, 0.43025, 0.23951724137931032, 0.14705625, 0.36775468749999995, 0.3116328125, 0.3673964912280702, 0.10132222222222223, 0.2696722222222222, 0.23618888888888886, 0.54715, 0.20095, 0.0, 0.69225, 0.16635, 0.16071111111111114, 0.5221999999999999, 0.3414545454545455, 0.4943666666666667, 0.198175, 0.0, 0.37677272727272726, 0.3316142857142857, 0.6830666666666666, 0.6588, 0.32454, 0.3818, 0.7262, 0.19609166666666666, 0.65662, 0.48405, 0.11938750000000001, 0.8382499999999999, 0.2257, 0.45954000000000006, 0.0, -0.0009333333333333323, 0.2381, -0.0516, 0.21255000000000002, 0.0, 0.836, 0.2798909090909091, 0.2263, 0.21134, 0.665575, 0.6801333333333334, 0.2781875, 0.6278545454545454, 0.6249, 0.65235, 0.0, 0.37595, 0.765, 0.5859, 0.6479666666666667, 0.2732, 0.8779, 0.43895, 0.4806333333333333, 0.5391, 0.31845, 0.347675, 0.6249, 0.70605, 0.8316, 0.4201125, 0.6716500000000001, 0.28568, 0.5377, -0.2755, 0.14448, 0.0, 0.19063333333333332, 0.36331250000000004, 0.08654999999999997, 0.6249, 0.872, 0.28595, 0.5719, 0.2997, 0.0, 0.5727333333333334, -0.1548, 0.30258888888888896, 0.6361, 0.6152333333333334, 0.5840000000000001, 0.5349333333333334, -0.1147, 0.212775, 0.7639, 0.10796666666666667, 0.0, 0.5771, 0.0, 0.2441111111111111, 0.1918, 0.1918, -0.033, 0.2622, 0.128, 0.17346363636363635, 0.5927, 0.5927, 0.4199, -0.2755, 0.07916000000000001, 0.07916000000000001, 0.5994, 0.0, 0.6369, 0.6239, -0.2584, 0.8374, 0.836, 0.6948, 0.6948, 0.271025, 0.0, 0.5346, 0.4404, 0.31245, -0.6114, 0.5574, 0.3275666666666667, -0.09106666666666667, 0.4404, -0.2732, 0.4404, 0.371225, 0.6940333333333332, 0.4939, 0.8126, 0.4767, 0.0, 0.41355, 0.5106, 0.4215, 0.2949, 0.0903, 0.67865, 0.0, 0.0, 0.3877, 0.4588, 0.0, 0.4939, 0.28595, 0.4927, 0.2844, 0.0, 0.8481, 0.0, 0.0772, 0.7574, 0.49905, 0.3612, 0.7269, 0.61165, 0.0, 0.791, 0.3612, 0.0, 0.9145, 0.0258, 0.21230000000000002, 0.41785, 0.3818, 0.42603333333333343, 0.3182, 0.55465, 0.0, 0.0, 0.6114, 0.0, 0.0, 0.5719, 0.9538, 0.0, 0.0, 0.6369, 0.65585, -0.2222, 0.4201, 0.0, 0.0, 0.6808, 0.07655, 0.18332857142857142, 0.012849999999999973, 0.5994, 0.26380000000000003, -0.2732, 0.2787, 0.34, 0.0, -0.3252, 0.0, 0.3045, 0.624, 0.0, 0.4767, 0.4215, 0.6114, 0.5563, 0.4404, 0.5984, 0.4832666666666667, 0.5267, -0.5409, 0.8225, 0.0, 0.0, 0.8126, 0.8126, -0.24695, 0.5719, 0.0, 0.7645, 0.0, -0.50455, 0.6956, -0.2732, 0.4404, 0.4926, 0.19298, 0.8433, 0.5994, 0.0, 0.5962, 0.5994, 0.34005, 0.23607499999999998, 0.0, 0.0, -0.019500000000000017, 0.8176, 0.0, -0.296, -0.5093, 0.128, 0.0, 0.4939, 0.4497, 0.4497, 0.0, 0.5076, 0.9412, 0.6166, 0.7036, 0.7424, 0.5423, 0.8442, 0.0, 0.8126, 0.8126, 0.6637, 0.0772, 0.78735, 0.8176, 0.0, 0.0, 0.1591, 0.0, 0.8695, -0.636, 0.0, 0.31245, 0.6249, 0.6249, 0.6249, 0.2732, 0.0, 0.0, 0.0, 0.6115, 0.1191, 0.6124, -0.5574, 0.7003, 0.7775, -0.29185, 0.0, -0.1779, 0.659, 0.4588, 0.0, 0.6977, 0.0, 0.7906, 0.5106, 0.0, 0.5859, 0.4215, -0.128, 0.0, 0.9042, 0.0, 0.4588, -0.1761, 0.6588, 0.0, 0.4215, 0.7384, 0.6688, 0.8306, 0.4574, 0.3612, -0.1263, 0.3612, 0.4215, 0.0, 0.0, 0.2382, 0.0]})\n",
      "Rankings result: \n",
      "        entity  average_stars  predicted_score\n",
      "0         food       3.888521         0.465200\n",
      "1      service       3.959103         0.554775\n",
      "2        place       3.937050         0.377412\n",
      "3         menu       4.026415         0.360981\n",
      "4       dinner       4.055351         0.357749\n",
      "5        pizza       3.904573         0.324784\n",
      "6        price       3.785530         0.380288\n",
      "7   restaurant       3.865784         0.378223\n",
      "8        staff       4.228571         0.517056\n",
      "9        pasta       3.835979         0.340938\n",
      "10      burger       3.974747         0.384439\n",
      "11      cheese       4.008451         0.239517\n",
      "12     chicken       3.959375         0.311633\n",
      "13        meal       3.955556         0.367396\n",
      "14       salad       3.980769         0.331614\n",
      "Spearman Correlation Score:  -0.11785714285714284\n",
      "Running on restaurant  IMLrj2klosTFvPRLv56cng ...\n",
      "Number of Reviews left after subset length:  679\n",
      "Extracting entities from each review...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering entities to have enough reviews...\n",
      "Calculating Yelp Star Rankings... \n",
      "Calculating Prediction Rankings...\n",
      "Performing sentiment analysis for each review... \n",
      "defaultdict(<class 'list'>, {'entity': ['peppers', 'steak', 'cheese', 'Service', 'tuna', 'corn', 'place', 'decor', 'food', 'bread', 'service', 'restaurant', 'price', 'sandwich', 'Brookie', 'fast', 'breakfast', 'boyfriend', 'chicken', 'salmon', 'staff', 'atmosphere', 'salad', 'Backyard', 'meal', 'burger', 'diner', 'menu', 'Food', 'bit', 'cream', 'friend', 'dinner', '$', 'neighborhood', 'freshly', 'pizza', 'prices', 'starters', 'calamari', 'dessert', 'table', 'pumpkin', 'dish', 'busser', 'brunch', 'problem', 'portion', 'run', 'side', 'busboy', 'quinoa', 'lunch', 'Enjoy', 'broccolini', 'Atmosphere', 'sprouts', 'mac', 'filet', 'meatloaf', 'booze', 'brussel', 'crispy', 'seasonal', 'office', 'Young', 'Salad', 'brown', 'potato', 'mash', 'cauliflower', 'box', 'pictures', 'meatball', 'beer', 'chef', 'toast', 'hash', 'plethora', 'plaza', 'Staff', 'craft', 'scallops', 'pricey', 'green', 'juice', 'revolutionary', 'skin', 'cardboard', 'brownie', 'extra', 'beet', 'Summerlin', 'butter', 'brookie', 'bite', 'truffle', 'burrata', 'date', 'plate', 'farm', 'Highly', 'soap', 'bartenders', 'pea', 'pure', 'fish', 'brick', 'extras', 'Fish', 'sergio', 'corner', 'seafood', 'owner', 'facelift', 'bundt', 'sugar', 'paces', 'waiter', 'feels', 'Carmel', 'turkey', 'sweet', 'road', 'Uber', 'decor', 'cookie', 'bass', 'orange', 'pomegranate', 'bun', 'flavor', 'course', 'vibe', 'blood', 'swordfish', 'specials', 'combo', 'bartender', 'courtesy', 'beard', 'party', 'power', 'cucumber', 'vend', 'buck', 'bottomless', 'clams', 'container', 'manager', 'flat', 'lamb', 'marinara', 'buttermilk', 'biscuits', 'Fatima', 'taste', 'sandwiches', 'margarita', 'wine', 'design', 'chip', 'chocolate', 'servicer', 'octopus', 'batter', 'pork', 'grill', 'ceasar', 'cocoa', 'greens', 'birthday', 'coffee', 'superb', 'delish', 'park', 'buffalo', 'Couch', 'foodie', 'restrooms', 'balcony', 'international', 'review', 'upscale', 'Lunch', 'Price', 'crab', 'casual', 'smaller', 'ricotta', 'biscuit', 'caeser', 'customer', 'choice', 'eggs', 'Scallops', 'snapper', 'surprise', 'pleasant', 'visual', 'Brookies', 'backyard', 'fennel', 'chickpea', 'fries', 'bomb', 'sangria', 'white', 'desserts', 'banana', 'cold', 'entree', 'meatballs', 'pasta', 'apps', 'dishes', 'wines', 'tasting', 'salt', 'Place', '&', 'lobster', 'mac&cheese', 'phenomenal', 'purchase', 'dress', 'bottom', 'shrimp', 'frittata', 'sauce', 'offerings', 'presentable', 'dcor', 'blast', 'charm', 'recommendation', 'Salmon', 'peppadew', 'chimichurri', 'brioche', 'slaw', 'Benedict', 'Chef', 'cole', 'mustard', 'suggestion', 'point', 'Brioche', 'hype', 'crust', 'prosciutto', 'Brunch', 'Mac', 'cafes', 'buddy', 'finale', 'banquet', 'foods', 'salads', 'servings', 'marscapone', 'Burger', 'beef', 'drive', 'brussels', 'venue', 'rice', 'servers', 'brownies', 'jerk', 'Sundays', 'sween', 'cury', 'spicy', 'french', 'flavors', 'strawberry', 'beers', 'spot', 'tomato', 'south', 'fall', 'steakhouse', 'peaches', 'comfy', 'manger', 'advice', 'slug', 'whole', 'Upscale', 'smallish', 'space', 'peak', 'daily', 'flatbread', 'real', 'setting', 'taster', 'stamina', 'Table', 'chefs', 'bunch', 'vote', 'risotto', 'solid', 'cool', 'absolute', 'cutest', 'scene', 'chutney', 'farmers', 'sucker', 'special', 'stood', 'squash', 'Bartender', 'light'], 'predicted_score': [0.20095, 0.3742444444444445, 0.30764722222222224, 0.39661333333333343, 0.2650090909090909, 0.192925, 0.5155099999999999, 0.5949015873015874, 0.5294085106382979, 0.28397857142857147, 0.4986378260869565, 0.3891173913043478, 0.17275652173913042, 0.34113469387755097, 0.69375, 0.7526666666666667, 0.3739952380952381, 0.636, 0.3286731707317074, 0.39446250000000005, 0.666166, 0.5281966666666666, 0.38462727272727265, 0.0, 0.45294000000000006, 0.3089541666666667, 0.0, 0.3237247706422018, 0.33015081967213117, 0.2213666666666667, 0.19452727272727274, 0.6681, 0.3962230769230769, 0.2263, 0.018699999999999994, 0.6249, 0.39747142857142864, 0.144575, -0.1007, -0.05035, 0.42902500000000005, 0.3859666666666667, 0.25487499999999996, 0.32454210526315785, 0.0, 0.4495375, -0.4019, -0.06434999999999999, 0.0, 0.30927499999999997, 0.4404, 0.396, 0.21494375000000002, 0.5411, 0.4404, 0.46245714285714284, 0.41975999999999997, -0.68, 0.332175, 0.44535, 0.24635, 0.7901, 0.7901, 0.7713, 0.7178, 0.4939, 0.71095, 0.09689999999999999, 0.54785, 0.6369, 0.15265, 0.0, -0.2617, 0.47396666666666665, 0.5657714285714286, 0.2083, 0.1920375, 0.0, 0.0, 0.27495, 0.42604999999999993, 0.36345, 0.3182, 0.89125, 0.4375, 0.578375, 0.9239, 0.731, 0.8558, 0.25055, 0.836, 0.0, 0.4275166666666667, 0.4441800000000001, 0.29642, 0.69475, 0.50214, 0.31379999999999997, 0.48996666666666666, 0.11738000000000001, 0.6478, 0.44168, 0.0, 0.886, 0.0, -0.2732, 0.38295714285714283, 0.7351, -0.5267, 0.7906, 0.7163, 0.0, 0.09198333333333335, 0.30025, 0.0, 0.4989, 0.7005, -0.5862, 0.6588, -0.5862, 0.4201, 0.340975, 0.5886800000000001, 0.7906, 0.0, 0.6688, 0.14343333333333333, -0.2755, 0.3014, 0.4754, 0.24695, -0.0772, 0.44756666666666667, 0.8313, 0.2524666666666667, 0.65935, 0.4404, 0.7959499999999999, 0.75958, 0.0258, 0.6808, 0.128, 0.20548333333333332, 0.4199, 0.2732, 0.48795, 0.43479999999999996, 0.0, 0.0, 0.44562999999999997, -0.318, 0.4561, 0.496, 0.49607999999999997, 0.0, 0.6369, 0.3182, -0.1232, 0.3166333333333333, 0.49529999999999996, 0.6369, 0.0, 0.3634, 0.7717, 0.0, 0.0, 0.21378, 0.5994, 0.7506, 0.6249, 0.6239, 0.30533333333333335, 0.7096, 0.87195, 0.184, 0.2255, 0.3612, 0.8999, 0.43865, 0.0, 0.0, 0.75, 0.02555, 0.53855, 0.6588, 0.4772, 0.8077, 0.4626, 0.7269, 0.6738, 0.7269, 0.6808, 0.7664666666666667, 0.4555, 0.0, 0.0, 0.0, 0.6799999999999999, 0.6819000000000001, 0.0, 0.0, 0.0, -0.1531, -0.2755, 0.44173333333333337, 0.6124, 0.1591, 0.5927, 0.7096, 0.3182, 0.0, 0.35198000000000007, 0.5859, 0.0, 0.2732, 0.5654, 0.6369, 0.0, 0.0, 0.39225, 0.0, 0.7269, 0.6369, 0.0, 0.5106, 0.8039, 0.0, 0.3761333333333334, 0.0, 0.0, 0.2732, 0.0, 0.6369, 0.0, 0.7351, 0.24695, 0.0, 0.0, 0.0, 0.4767, 0.5869333333333334, 0.0, 0.0, 0.8074, 0.8074, 0.4767, -0.1695, 0.0, 0.20383333333333334, 0.5719, 0.7284, 0.6245, 0.2156, 0.802, -0.1779, 0.8316, 0.0, 0.3182, 0.6239, 0.7086, 0.0, 0.296, 0.0, 0.5267, 0.8516, 0.3612, 0.0, 0.49305, 0.8313, -0.34, 0.3612, 0.0, 0.3182, 0.0, -0.5994, 0.0, 0.0, 0.6149, 0.6369, 0.4201, 0.0, 0.7506, 0.875, 0.0, 0.0, -0.2263, 0.8952, 0.0, 0.0, 0.0, 0.0, 0.6588, 0.6588, 0.508, 0.5106, 0.0, 0.4215, 0.4303, 0.296, 0.6588, 0.0, 0.0, 0.5526, 0.0, 0.1531, 0.8519, 0.8225, 0.8519, 0.2732, 0.0, 0.0, -0.6808, -0.3716, 0.296, 0.4939, -0.4648, 0.3802]})\n",
      "Rankings result: \n",
      "        entity  average_stars  predicted_score\n",
      "0       cheese       3.900000         0.307647\n",
      "1      service       3.901774         0.498638\n",
      "2        place       3.953243         0.515510\n",
      "3        decor       4.022388         0.594902\n",
      "4         food       3.883984         0.529409\n",
      "5   restaurant       3.853608         0.389117\n",
      "6     sandwich       3.978541         0.341135\n",
      "7      chicken       3.908072         0.328673\n",
      "8        staff       4.008584         0.666166\n",
      "9   atmosphere       4.210084         0.528197\n",
      "10       salad       3.972067         0.384627\n",
      "11        meal       3.700389         0.452940\n",
      "12        menu       3.869658         0.323725\n",
      "13      dinner       3.978056         0.396223\n",
      "Spearman Correlation Score:  0.4241758241758242\n",
      "Running on restaurant  ujHiaprwCQ5ewziu0Vi9rw ...\n",
      "Number of Reviews left after subset length:  1698\n",
      "Extracting entities from each review...\n",
      "Filtering entities to have enough reviews...\n",
      "Calculating Yelp Star Rankings... \n",
      "Calculating Prediction Rankings...\n",
      "Performing sentiment analysis for each review... \n",
      "defaultdict(<class 'list'>, {'entity': ['crab', 'dinner', 'buffet', 'stood', 'price', 'specialty', 'breakfast', 'place', 'food', 'buck', 'bang', 'ravioli', 'temp', 'breaks', 'Buffett', 'restaurant', 'sweet', 'buzz', 'service', 'lack', 'cocktail', 'picky', 'coffee', 'flank', 'seafood', 'fish', 'finest', 'flavorless', 'decor', 'bit', 'staff', 'pleasant', 'Service', 'beverage', 'table', 'sads', 'weekend', 'cleanest', 'pastas', 'chinese', 'sushi', 'reviews', 'lunch', 'Food', 'beef', 'bass', 'dessert', 'meal', 'corn', 'chicken', 'chef', 'pizza', 'creme', 'meat', 'brunch', 'tuna', 'beer', 'options', 'line', 'salmon', 'juice', 'smoky', 'Express', 'fruit', 'terrible', 'atmosphere', 'canteen', 'Staff', '100x', 'Aria', 'channel', 'recommendation', 'program', 'trip', 'system', 'pesto', 'salad', 'cheese', 'dish', 'pork', 'Highly', 'palace', 'special', 'bunch', 'pricey', 'frozen', 'customer', 'mood', 'advice', 'Kinda', 'bellagio', 'flavoring', 'strawberries', 'chocolate', 'team', 'bathrooms', 'servers', 'flan', 'brle', 'swordfish', 'rib', 'rack', 'pasta', 'scale', 'speak', 'comps', '10am-12pm', 'champagne', 'eggs', 'plate', 'penny', 'liquor', 'cashiers', 'Mediocre', 'waiter', 'DESSERTS', 'choice', 'Buffet', 'side', 'omelette', 'BREAKFAST', 'motel', 'roast', 'deserts', 'picatta', 'slot', 'selections', 'truffle', 'foods', 'steak', 'soft', 'exit', 'bartenders', 'bomb', 'slice', 'short', 'register', 'shellfish', 'sashimi', 'Seahawks', 'shaby', 'caviar', 'particular', 'buddy', 'peak', 'desserts', 'cream', 'california', 'mimosas', 'hype', 'plethora', 'part', 'sheer', 'crabs', 'lettuce', 'prime', 'slight', 'bummer', 'cafeteria', 'shrimps', 'menu', 'mediocre', 'fast', 'lobster', 'dishes', 'tortelini', 'bruele', 'shrimp', 'lyonais', 'NONE', 'reminder', 'sehr', 'prices', 'standard', 'rice', 'beverages', 'Aaaaand', 'places', 'casinos', 'desert', 'Breakfast', 'esta', 'ciudad', 'paisanos', 'mousse', 'positive', 'deal', 'premier', 'Price', 'bowls', 'Bate', 'slop', 'spinach', 'freestyle', 'convenience', 'Bartender', 'Rib', 'doubt', 'chefs', 'flem', 'KIng', 'cleaner', 'restrooms', 'skinny', 'runny', 'crack', 'usual', 'sport', 'kitsch', 'DESERTS', 'lines', 'Liquor', 'varies', 'airport', 'spareribs', 'sirloin', 'buffalo', 'plump', 'bone', 'fair', 'shorter', 'entree', 'sauce', 'buffett', 'spice', 'bien', 'buena', 'offerings', 'Solid', 'toast', 'extra', 'bacon', 'pastures', 'main', 'friend', 'lamb', 'vegetarians', 'Auswahl', 'pace', 'obese', 'feast', 'seaweed', 'sewer', 'gourmet', '&', 'vegan', 'starches', 'finish', 'taste', 'ventilation', 'steal', 'gooood', 'Lamb', 'comp', 'civiche', 'stuffs', 'sevice', 'super', 'kobe', 'ordinary', 'palate', 'boyfriend', 'delish', 'muffin', 'Stood', 'Station', 'freshness', 'cheesecakes', 'street', 'switch', 'Enjoy', 'patron', 'spicy', 'sauces', 'one', 'potato', 'crawfish', 'manager', 'goose', 'ribs', 'power', 'fact', 'Sundays', 'sandwich', 'point', 'juices', 'discount', 'cooked', 'thai', 'lobsters', 'slightl', 'butter', 'snippy', 'kids', 'asparagus', 'cutesy', 'suckers', 'Thanksgiving', 'shitty', 'downside', 'savory', '35-ish', 'brisket', 'carving', 'last', 'Lil', 'belligio', 'buns', 'burger', 'Lunch', 'waitstaff', 'hefty', 'coca', 'cola', 'shit', 'space', 'sugar', 'expresso', 'bread', 'stick', 'Salmon', 'front', 'moon', 'season', 'spread', 'casino', 'bottomless', 'conservatory', 'fuss', 'pastries', 'pricier', 'towel', 'biscuits', 'pancakes', 'ballagio', '45mins-1hr', 'brand', 'review', 'petite', 'carpet', 'road', 'flakey', 'Seafood', 'revs', 'clam', 'millions', '5-star', 'Atmosphere', 'buffet(Paris', 'place--', 'cakes', 'priciest', 'alotta', 'cultures', 'cares', 'sign', 'Corral', 'smoked', 'elsewehre', 'green', 'christian', 'problem', 'bussers', 'name', 'variety', 'Beef', 'Chocolate', 'tortilla', 'cotta', 'peal', 'denny', 'fondue', 'sunday', 'bright', 'park', 'weekends', 'pineapples', 'stole', 'gelato', 'past', 'birthday', 'lima', 'crews', 'middleeast', 'surface', 'giant', 'forEVER', 'brick', 'tips', 'Mimosas', 'stringy', 'crowds', 'dollar', 'value', 'benchmark', 'normal', 'Overcooked', 'banana', 'pues', 'absolute', 'coma', 'cash', 'xD', 'nutella', 'multigrain', 'diet', 'closer', 'hollywood', 'smaller', 'Fish', 'downer', 'perks', 'slow', '11:30am', 'ceviche', 'french', 'container', 'rejoice', 'fries', 'prison', 'foster', 'bananas', 'decorations', 'margarita', 'poppin', 'ceilings', 'selection', 'bite', 'dirty', 'stall', 'families', 'pack', 'Shellfish', 'Belagio', 'middle', 'elegant', 'animals', 'sacrifice', 'foodie', '$', 'cake', 'mimosa', 'RIB', 'ones', 'jumbo', 'berry', '25minutes', 'Tip', 'bland', 'combo', 'steht', 'Schlange', 'Romantiker', 'property', '35059', 'aria', 'sweets', 'mash', 'pickiest', 'milk', 'platter', '48+tax', 'setup', 'Resort', 'rosemary', 'brulee', 'crepes', 'bagel', 'ceasar', 'formica', 'Alexander', 'superb', 'uniformly', 'curie', 'bartender', 'cold', 'PH'], 'predicted_score': [0.16115940959409597, 0.16227575757575757, 0.2498041518386714, 0.08165789473684211, 0.20273724696356277, -0.31245, 0.23827438016528923, 0.19771367924528302, 0.22719149746192896, 0.08626666666666667, 0.14282, 0.5504, -0.07144999999999999, 0.1531, -0.15806666666666666, 0.2558346153846154, 0.3016944444444445, -0.12055, 0.33862857142857145, -0.128, 0.22667272727272725, -0.10345, 0.5558166666666667, 0.54785, 0.2708397959183673, 0.20098235294117645, 0.0, 0.06264285714285714, 0.28873076923076924, 0.21961250000000002, 0.3665, 0.8834, 0.23835357142857141, 0.0, 0.13198695652173914, 0.0, 0.33335000000000004, 0.5859, 0.2051333333333333, 0.05205, 0.14770408163265303, 0.0, 0.10799999999999998, 0.16147529411764708, 0.21667999999999996, 0.40359999999999996, 0.34718571428571426, 0.30893225806451613, 0.5994, 0.1451761904761905, 0.2892125, 0.10017368421052633, 0.0, 0.1638, 0.17403809523809524, 0.47172000000000003, 0.17615000000000003, 0.0, 0.11930689655172415, 0.3059697674418605, 0.39005714285714294, 0.3612, 0.0, -0.014133333333333331, -0.4763, 0.44716666666666666, 0.081, 0.45005, 0.5097999999999999, 0.3599, 0.0, 0.23835, 0.4767, 0.36755, 0.0, 0.2202, 0.2579913043478261, 0.15845833333333334, 0.24341111111111108, 0.19176666666666667, 0.466025, 0.30432000000000003, 0.11338749999999997, 0.40080000000000005, 0.4529777777777778, -0.06605000000000001, 0.681, -0.2585, -0.05135, 0.15205, 0.14867333333333332, -0.1548, 0.2733285714285714, 0.2992444444444444, 0.56775, 0.714, 0.2514, 0.3089, 0.25984, 0.0, 0.23317142857142853, 0.2733333333333334, 0.23828275862068968, -0.3785, 0.0, 0.479375, 0.0, 0.5326444444444445, 0.10206153846153845, 0.3847833333333333, 0.3601500000000001, -0.43674999999999997, 0.0, 0.0, 0.3182, 0.6369, 0.41420000000000007, 0.19165806451612907, 0.20168, 0.40675, 0.0, 0.3612, 0.6373, 0.7127, 0.5267, 0.0, 0.08478571428571431, -0.4588, -0.03880000000000001, 0.15680588235294118, 0.08158000000000001, 0.0772, 0.762, 0.6124, 0.2917, 0.5859, 0.85155, 0.0, 0.3429666666666666, 0.0, 0.0, 0.45389999999999997, 0.3437666666666666, -0.2263, 0.7964, 0.38286, -0.0415909090909091, 0.3359, 0.0, 0.15953, 0.31485, 0.6044, 0.4095, 0.2623666666666667, 0.3182, 0.2832, -0.5719, -0.3818, 0.4428666666666666, 0.4156333333333333, 0.2676117647058824, 0.0, 0.21075, 0.695, 0.15846666666666667, 0.0, 0.6369, 0.23412000000000002, 0.0, 0.0, -0.296, -0.4158, 0.17943636363636364, 0.0, 0.09879999999999997, -0.19816666666666669, 0.0, 0.0, 0.0, 0.5353333333333333, 0.24332857142857142, -0.3229, 0.0, -0.296, -0.014366666666666675, 0.5574, 0.25966000000000006, 0.0, 0.13664375, 0.1806, 0.0, 0.0, 0.40515, 0.43910000000000005, 0.4215, 0.4754, 0.3601, 0.7438, 0.742, 0.5267, 0.0, -0.2942, -0.2942, -0.1027, -0.027350000000000013, 0.23459999999999998, 0.1752, 0.0, 0.6249, 0.8622, -0.005180000000000007, -0.1027, 0.0, -0.6908, -0.6799, 0.28595, 0.5719, 0.5719, 0.0, 0.7351, 0.1468, -0.2997, -0.05300000000000001, 0.28926666666666667, 0.10946666666666667, -0.6458, -0.37273333333333336, 0.2498, 0.1531, 0.5830333333333334, 0.0772, 0.22293333333333334, 0.0, 0.26089999999999997, 0.4939, 0.5101818181818183, 0.743, -0.8316, 0.4201, 0.6249, 0.6249, 0.3237, 0.3612, 0.0, 0.23349999999999999, 0.0, 0.7553, 0.06534999999999999, -0.011633333333333329, 0.4404, -0.5411, 0.0, 0.4404, 0.4303, 0.6249, 0.296, -0.8338, 0.8622, 0.52175, 0.0, 0.4588, 0.34845, 0.3802, 0.7096, 0.0, 0.807, 0.6124, 0.6114, 0.0, 0.4588, 0.5837, 0.391, 0.6249, 0.0, 0.29635, 0.5859, -0.22835000000000003, -0.1892, 0.6166, 0.296, 0.0, 0.19485, 0.0, 0.214, 0.03599999999999999, 0.5106, 0.3182, 0.6841, 0.0, 0.0, 0.0, 0.16406, 0.0, 0.0, 0.0, 0.802, -0.7269, 0.0, -0.7901, -0.16555, 0.2263, 0.6486, 0.0482, 0.09196666666666665, 0.0, 0.6124, 0.6249, 0.5719, -0.197, 0.0, 0.0945, 0.1901, 0.6369, 0.6369, -0.6428, 0.3612, 0.6754, 0.1531, 0.18023333333333333, 0.6486, 0.2871, 0.22096666666666667, 0.2382, 0.0, 0.75, 0.5719, 0.29567499999999997, 0.0, 0.0, 0.06066666666666667, 0.20095, 0.0, 0.0, 0.6249, 0.2263, 0.4194, -0.056599999999999984, 0.0, 0.6197, 0.0, 0.0, 0.0, 0.159925, 0.0, 0.4404, 0.0, 0.2382, 0.7755, -0.5267, 0.0, 0.296, 0.6486, 0.6486, 0.6486, 0.4245, 0.148, 0.6633, 0.4294, 0.0, 0.0, 0.6249, -0.7096, -0.4404, 0.2202, 0.0, 0.4037, 0.5972, 0.296, 0.0, -0.7043, 0.3041, 0.0, -0.3802, 0.4404, 0.9049, 0.7964, 0.0964, 0.0, 0.2401, 0.2202, 0.705, 0.0, 0.4939, 0.3612, 0.6826, 0.0, -0.1027, 0.0, 0.3612, 0.7506, 0.0, 0.5859, 0.5719, 0.5719, 0.0, 0.4019, 0.0, 0.6369, 0.0, 0.6696, 0.0, 0.4404, 0.5859, 0.0, 0.0, 0.296, 0.4588, 0.4404, -0.296, 0.0, -0.296, -0.0772, -0.3412, 0.0, -0.5423, 0.0, 0.7096, 0.743, -0.3595, -0.5106, -0.4767, -0.4767, 0.6705, -0.6249, 0.2075, -0.4482, 0.56525, 0.4588, -0.1779, -0.2023, 0.68, 0.0, 0.0, 0.4939, 0.6124, 0.4767, -0.3657, 0.0, -0.23835, 0.0, 0.5229, -0.3167, 0.0, 0.7263, 0.5423, -0.5423, 0.0, 0.0685, 0.7279, -0.1779, 0.5994, 0.5994, 0.0, 0.0, 0.0, 0.0, -0.6577, 0.8172, 0.2942, 0.5079, 0.0, 0.0, 0.0, 0.0, 0.5574, 0.5719, 0.6557, 0.3612, 0.9109, 0.6597, 0.1779, 0.7964, 0.6249, -0.4939, 0.3182, 0.0, 0.4902]})\n",
      "Rankings result: \n",
      "       entity  average_stars  predicted_score\n",
      "0        crab       3.396911         0.161159\n",
      "1      dinner       3.526952         0.162276\n",
      "2      buffet       3.399582         0.249804\n",
      "3       price       3.330591         0.202737\n",
      "4   breakfast       3.678261         0.238274\n",
      "5       place       3.265097         0.197714\n",
      "6        food       3.294837         0.227191\n",
      "7     service       3.393973         0.338629\n",
      "8     seafood       3.586146         0.270840\n",
      "9         bit       3.510909         0.219613\n",
      "10      staff       3.147766         0.366500\n",
      "11      sushi       3.466799         0.147704\n",
      "12      lunch       3.677989         0.108000\n",
      "13       beef       3.603960         0.216680\n",
      "14    dessert       3.586803         0.347186\n",
      "15       meal       3.264059         0.308932\n",
      "16     salmon       3.537530         0.305970\n",
      "Spearman Correlation Score:  -0.2034313725490196\n",
      "Running on restaurant  OVTZNSkSfbl3gVB9XQIJfw ...\n",
      "Number of Reviews left after subset length:  561\n",
      "Extracting entities from each review...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering entities to have enough reviews...\n",
      "Calculating Yelp Star Rankings... \n",
      "Calculating Prediction Rankings...\n",
      "Performing sentiment analysis for each review... \n",
      "defaultdict(<class 'list'>, {'entity': ['buffet', 'crab', 'place', 'RIB', 'food', 'Service', 'cocktail', 'concentrate', 'machine', 'juice', 'cheese', 'chicken', 'pizza', 'price', 'sashimi', 'cookies', 'gelato', 'sweet', 'breakfast', 'juices', 'chance', 'bargain', 'dinner', 'meat', 'Food', 'beer', 'entree', 'dessert', 'part', 'plate', 'service', 'coupon', 'cuisine', 'brunch', 'choice', 'bartender', 'seafood', 'staff', 'freezer', 'oatmeal', '$', 'Breakfast', 'meal', 'front', 'restaurant', 'places', 'lunch', 'point', 'smoked', 'salmon', 'line', 'chinese', 'pork', 'inclusive', 'freshness', 'endless', 'pasta', 'donuts', 'mimosas', 'decor', 'Nice', 'weekend', 'tix4tonight', 'sauce', 'desert', 'coctail', 'Buffet', 'spectrum', 'mediocre', 'extra', 'froyo', 'machines', 'bagel', 'sandwich', 'deal', 'weekends', 'salad', 'Price', 'rien', 'real', 'scale', 'free', 'bit', 'regular', 'nice', 'Crab', 'Rib', 'burger', 'shrimp', 'penny', 'mushrooms', 'spinach', 'champagne', 'cuisines', 'menu', 'downside', 'premium', 'section', 'sushi', 'guacamole', 'seller', 'main', 'picky', 'potatos', 'rice', 'Aria', 'stick', 'wine', 'chef', 'rib', 'discount', 'side', 'past', 'slice', 'beef', 'coma', 'TVs', 'cafeteria', 'casino', 'solid', 'duck', 'palatable', 'stood', 'usual', 'table', 'roast', 'seaweed', 'biggest', 'prime', 'bay', 'joke', 'frozen', 'atmosphere', 'fruit', 'foods', 'pluss', 'cruise', 'broad', 'Highly', 'soft', 'Sign', 'turkey', 'brle', 'shrimps', 'bread', 'sugar', 'cream', 'prices', 'comps', 'frat', 'pantry', 'comp', 'ID', 'cappuccino', 'Buffett', 'taste', 'deserts', 'Advice', 'salami', 'special', 'booze', 'perspective', 'surface', 'desserts', 'congele', 'shirmp', 'pass', 'summer', 'properties', 'burrito', 'beverage', 'salsa', 'brown', 'toast', 'french', 'brighter', 'pumpkin', 'blondies', 'buzz', 'brule', 'crack', 'squeeze', 'chocolate', 'grace', 'Beer', 'screen', 'eggs', 'Staff', 'prim', 'short', 'carving', 'offerings', 'jumbo', '24/head', 'basics', 'mousse', 'sticky', 'cola', '8:30am', 'shortest', 'serve', 'compares', 'waitor', 'friend', 'pricey', 'sheet', 'soda', 'salads', 'Variety', 'draft', 'manager', 'omlette', 'bomb', 'shellfish', 'dish', 'omelette', 'spicy', 'fact', 'servers', 'strip', 'mexican', 'reviews', 'cakes', 'clam', 'california', 'ceasars', 'brisket', 'bottomless', '6ish', 'dilemma', 'buena', 'carnes', 'bland', 'remembers', 'buns', 'cinnamon', 'produce', 'watermelon', 'fish', 'ones', 'alfredo', 'fries', 'sugarholic', 'bunch', 'cashiers', 'space', 'Solid', 'Nevermind', 'beers', 'benefit', 'stew', 'Money', 'green', 'beans', 'corner', 'Place'], 'predicted_score': [0.22696616541353384, 0.23879843750000002, 0.20697126436781607, 0.34, 0.24896735395189004, 0.3975578947368421, 0.20772, 0.0, 0.0, 0.284725, 0.5061333333333333, -0.07286000000000001, 0.3899, 0.28932115384615387, -0.148, 0.47196666666666665, 0.3528, 0.35743333333333327, 0.17698333333333335, 0.7096, -0.2663, 0.7096, 0.17840545454545453, 0.42008, 0.21925813953488374, 0.31285257731958765, 0.0, 0.29004117647058825, 0.7065666666666667, -0.05135, 0.4995109090909091, 0.02054, 0.2263, 0.4069857142857143, 0.4623333333333333, 0.46914285714285714, 0.2917619047619048, 0.5766130434782609, 0.0, 0.0, 0.1396, 0.270675, 0.28463125, 0.0, 0.32374545454545456, 0.7783, 0.07884000000000001, 0.0, -0.5267, 0.029300000000000038, 0.24695, 0.3837333333333333, 0.2038, 0.3612, -0.2755, 0.5574, 0.25764166666666666, 0.3612, 0.32256666666666667, 0.31429999999999997, 0.6705, 0.5326500000000001, 0.0772, 0.4404, -0.05849999999999998, 0.4404, 0.0, 0.0, 0.32926666666666665, 0.4404, -0.4019, 0.7783, 0.4648, 0.1938, 0.52766, 0.4404, 0.29048461538461534, 0.14205000000000004, 0.0, 0.3612, -0.296, 0.6069888888888889, -0.026716666666666666, 0.0, 0.6692, -0.148, 0.0073999999999999995, 0.0, 0.4166, 0.2263, 0.2263, 0.2263, 0.517375, 0.57575, 0.4927, 0.6614, 0.6614, -0.05065000000000003, 0.20722666666666667, 0.6211, 0.2263, 0.0, 0.0, 0.0, 0.16575, 0.15293333333333334, -0.0772, 0.19749999999999998, 0.58145, 0.4927, 0.6486, 0.08055, 0.3715, -0.296, 0.17045000000000002, 0.0, -0.4026, 0.26089999999999997, -0.24105, 0.1531, 0.1531, 0.68, 0.4965, 0.6249, 0.25646, 0.20095, 0.2732, 0.5859, 0.429075, 0.4404, 0.5423, 0.39135, 0.48550000000000004, 0.3182, -0.4158, 0.4019, 0.4926, 0.6478, 0.2716, 0.0, 0.0, 0.7442500000000001, 0.4927, -0.6202, 0.442125, 0.66395, 0.27243333333333336, 0.3716, 0.0, 0.0, 0.0, 0.21230000000000002, 0.0, 0.6588, 0.492, 0.03490000000000001, 0.0, 0.0, 0.0, 0.3408333333333333, 0.21230000000000002, 0.0, 0.6115, 0.14495, -0.1531, 0.0, 0.2382, 0.8225, 0.6682, 0.8579, 0.0, 0.0, 0.5563, 0.4404, 0.4404, 0.3818, 0.7964, 0.7964, 0.6808, 0.7773, 0.6249, 0.0, 0.0032999999999999974, 0.6808, 0.4215, 0.31245, 0.09954999999999997, 0.5153, 0.4404, 0.3818, 0.4961666666666667, 0.3818, 0.0, 0.0, 0.296, 0.1326, -0.5096, 0.0, -0.2844, -0.4939, 0.2023, -0.5267, 0.7249, 0.4939, 0.0, -0.2263, 0.0, 0.8016, 0.0, 0.6696, 0.1027, 0.8271, 0.8271, -0.4226, -0.2113, 0.5994, 0.6633, 0.0, 0.3893333333333333, 0.4404, 0.7181, 0.0, -0.1027, 0.0, -0.3901, -0.1759, -0.5994, 0.6588, 0.0, -0.1779, 0.0, 0.0, -0.4215, -0.4215, 0.2235, 0.2235, 0.0018, 0.0, 0.0, 0.6705, 0.0, 0.0, 0.0, 0.31845, -0.5859, 0.0, 0.1531, 0.3612, 0.0, 0.4588, 0.6369, 0.0772, 0.0, 0.0, 0.3612, 0.9022]})\n",
      "Rankings result: \n",
      "      entity  average_stars  predicted_score\n",
      "0     buffet       3.224237         0.226966\n",
      "1       crab       3.254795         0.238798\n",
      "2      place       3.154519         0.206971\n",
      "3       food       3.134766         0.248967\n",
      "4    service       3.179104         0.499511\n",
      "5      price       3.275229         0.289321\n",
      "6  breakfast       3.357759         0.176983\n",
      "7     dinner       3.231343         0.178405\n",
      "8       beer       3.394886         0.312853\n",
      "Spearman Correlation Score:  -0.016666666666666666\n",
      "Running on restaurant  HhVmDybpU7L50Kb5A0jXTg ...\n",
      "Number of Reviews left after subset length:  1600\n",
      "Extracting entities from each review...\n",
      "Filtering entities to have enough reviews...\n",
      "Calculating Yelp Star Rankings... \n",
      "Calculating Prediction Rankings...\n",
      "Performing sentiment analysis for each review... \n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "correlation_scores = []\n",
    "\n",
    "# Initialize predictor\n",
    "predictor = Predictor()\n",
    "\n",
    "for bus_id in tqdm(business_ids_similar_stars):\n",
    "    print(\"Running on restaurant \", bus_id, \"...\")\n",
    "    subset = bus[bus.business_id == bus_id]\n",
    "    \n",
    "    # only get reviews with enough amount of text\n",
    "    reviews_subset = [review for review in subset.text if len(review) < 400]\n",
    "\n",
    "    print(\"Number of Reviews left after subset length: \", len(reviews_subset))\n",
    "    \n",
    "    # get set of entities for this particular restaurant,\n",
    "    # and count how many reviews each entity have\n",
    "    entities_with_count = defaultdict(int) \n",
    "    review_entities = [] # extract entities for each review\n",
    "    print(\"Extracting entities from each review...\")\n",
    "    for review in reviews_subset:\n",
    "        entities = get_entities(nlp, review)\n",
    "\n",
    "        # add this review as a count to an entity\n",
    "        for ent in entities:\n",
    "            entities_with_count[ent.lower()] += 1\n",
    "\n",
    "        review_entities.append(entities)\n",
    "        \n",
    "    # only grab entities that have enough reviews\n",
    "    print(\"Filtering entities to have enough reviews...\")\n",
    "    entities_with_enough_reviews = []\n",
    "    threshold = 30\n",
    "    for key, value in entities_with_count.items():\n",
    "        if value >= threshold:\n",
    "            entities_with_enough_reviews.append(key)\n",
    "            \n",
    "    # TRUE RANKINGS CALCULATION\n",
    "    # for each entity, average ratings\n",
    "    true_rankings = defaultdict(list)\n",
    "\n",
    "    print(\"Calculating Yelp Star Rankings... \")\n",
    "    for entity in entities_with_enough_reviews:\n",
    "        true_rankings['entity'] += [entity]\n",
    "        entity_reviews = subset[subset.text.str.contains(entity, case=False)]\n",
    "        true_rankings['average_stars'] += [np.mean(entity_reviews.stars)]\n",
    "\n",
    "    true_rankings = pd.DataFrame(true_rankings)\n",
    "    \n",
    "    # PREDICTION RANKING CALCULATION\n",
    "    print(\"Calculating Prediction Rankings...\")\n",
    "    # Filter entities of each review to be from the entities_with_enough_review set\n",
    "    entity_filter = set(entities_with_enough_reviews)\n",
    "\n",
    "    filtered_entities = []\n",
    "\n",
    "    for entities in review_entities:\n",
    "        filtered = []\n",
    "        for ent in entities:\n",
    "            ent = ent.lower()\n",
    "            if ent in entity_filter:\n",
    "                filtered.append(ent)\n",
    "        filtered_entities.append(filtered)\n",
    "    \n",
    "    # perform sentiment analysis for each review with filtered entities above\n",
    "    predicted_scores = defaultdict(list)\n",
    "\n",
    "    print(\"Performing sentiment analysis for each review... \")\n",
    "    for i, review in enumerate(reviews_subset):\n",
    "        scores = predictor.customPredict(review, 9)\n",
    "        # save results \n",
    "        for entity, score in scores[0]:\n",
    "            predicted_scores[entity] += [score]\n",
    "\n",
    "    # create rankings from scores\n",
    "    predicted_rankings = defaultdict(list)\n",
    "    for entity, scores in predicted_scores.items():\n",
    "        predicted_rankings['entity'] += [entity]\n",
    "        predicted_rankings['predicted_score'] += [np.mean(scores)]\n",
    "    print(predicted_rankings)\n",
    "    predicted_rankings = pd.DataFrame(predicted_rankings)\n",
    "    #### may not be necessary to do these castings\n",
    "    predicted_rankings['entity'] = predicted_rankings['entity'].astype(str)\n",
    "    true_rankings['entity'] = true_rankings['entity'].astype(str)\n",
    "    ####\n",
    "    \n",
    "    full_rankings = true_rankings.merge(predicted_rankings, how='left').fillna(0)\n",
    "\n",
    "    # spearman correlation metric\n",
    "    print(\"Rankings result: \")\n",
    "    print(full_rankings)\n",
    "    \n",
    "    corr, pvalue = spearmanr(full_rankings.average_stars, full_rankings.predicted_score)\n",
    "    print(\"Spearman Correlation Score: \", corr)\n",
    "    correlation_scores.append(corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(correlation_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_bus = []\n",
    "for bus_id in tqdm(business_ids_similar_stars):\n",
    "    print(\"Running on restaurant \", bus_id, \"...\")\n",
    "    subset = bus[bus.business_id == bus_id]\n",
    "    \n",
    "    # only get reviews with enough amount of text\n",
    "    reviews_subset = [review for review in subset.text if len(review) < 400]\n",
    "\n",
    "    print(\"Number of Reviews left after subset length: \", len(reviews_subset))\n",
    "    \n",
    "    # get set of entities for this particular restaurant,\n",
    "    # and count how many reviews each entity have\n",
    "    entities_with_count = defaultdict(int) \n",
    "    review_entities = [] # extract entities for each review\n",
    "    print(\"Extracting entities from each review...\")\n",
    "    for review in tqdm(reviews_subset):\n",
    "        entities = get_entities(nlp, review)\n",
    "\n",
    "        # add this review as a count to an entity\n",
    "        for ent in entities:\n",
    "            entities_with_count[ent.lower()] += 1\n",
    "\n",
    "        review_entities.append(entities)\n",
    "        \n",
    "    # only grab entities that have enough reviews\n",
    "    print(\"Filtering entities to have enough reviews...\")\n",
    "    entities_with_enough_reviews = []\n",
    "    threshold = 30\n",
    "    for key, value in entities_with_count.items():\n",
    "        if value >= threshold:\n",
    "            entities_with_enough_reviews.append(key)\n",
    "            \n",
    "    # TRUE RANKINGS CALCULATION\n",
    "    # for each entity, average ratings\n",
    "    true_rankings = defaultdict(list)\n",
    "\n",
    "    print(\"Calculating Yelp Star Rankings... \")\n",
    "    for entity in entities_with_enough_reviews:\n",
    "        true_rankings['entity'] += [entity]\n",
    "        entity_reviews = subset[subset.text.str.contains(entity, case=False)]\n",
    "        true_rankings['average_stars'] += [np.mean(entity_reviews.stars)]\n",
    "\n",
    "    true_rankings = pd.DataFrame(true_rankings)\n",
    "    std_bus.append(np.std(true_rankings.average_stars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(std_bus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(std_bus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean =std_bus > np.mean(std_bus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import compress\n",
    "np.mean(list(compress(correlation_scores, boolean)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stars in full_rankings.average_stars:\n",
    "    print(np.std(stars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_subset[1023]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final Correlation Score: \", np.mean(correlation_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
